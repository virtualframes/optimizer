{"path": "Dockerfile", "size_bytes": 843, "lines": 26, "lang": "text", "sha256": "f5d6d11116fc7daa4607ca54d5b9b85f06568298cb69c4f527a373b1fae6266f", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "# Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy dependency checker script\nCOPY scripts/install_dependencies.sh /tmp/install_dependencies.sh\nRUN chmod +x /tmp/install_dependencies.sh\n\n# Install build dependencies required for pybullet and other packages\n# The script checks for missing dependencies and provides helpful suggestions\nRUN apt-get update && apt-get install -y build-essential && rm -rf /var/lib/apt/lists/*\n\n#"}
{"path": "Dockerfile.python", "size_bytes": 807, "lines": 25, "lang": "text", "sha256": "a9401eccb0d974ab21707ab6de13402c572b87af1f7da2ea91015bd0f1f76a95", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "# Stage 1: Builder\nFROM python:3.12-slim AS builder\nWORKDIR /app\n\n# Copy dependency checker script\nCOPY scripts/install_dependencies.sh /tmp/install_dependencies.sh\nRUN chmod +x /tmp/install_dependencies.sh\n\n# Install build dependencies required for pybullet and other packages\n# This includes g++, gcc, make, and other build essentials\nRUN apt-get update && apt-get install -y build-essential && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# S"}
{"path": "LICENSE", "size_bytes": 1055, "lines": 21, "lang": "text", "sha256": "3bed3331b7048bac17cf50e249d560ccc9508c970da8d7b9283bf4f2e633a91d", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "MIT License\n\nCopyright (c) 2025\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permissio"}
{"path": "README.md", "size_bytes": 3188, "lines": 110, "lang": "markdown", "sha256": "ec1f87e2bb51bdd9574e96fff2aba1e5c73378338abc0397b6c5fd02e6ddd7b8", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "# Optimizer\n\nAugmented optimizer for virtual node and game-engine authentication matrix simulation in a 3D spacetime physics environment.\n\nThis project provides a framework for simulating complex systems with a focus on:\n- **Virtual Node Simulation**: Core `Node` class for representing virtual simulation nodes.\n- **Physics Engine Integration**: An `Engine` adapter for PyBullet for 3D physics.\n- **Authentication Matrix**: An `AuthMatrix` module for node-to-node credential checks.\n- **REST API**: A FastAPI ba"}
{"path": "agentic_orchestration/agents/auto_debugger.py", "size_bytes": 422, "lines": 14, "lang": "python", "sha256": "b94cd081833fb87b3cb3aaf78c74273364ed4784a5e963fa79090d689608fb9e", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "\"\"\"\nPlaceholder for the auto-debugger agent.\nThis agent will be responsible for diagnosing issues in the orchestration flow.\n\"\"\"\n\nclass AutoDebugger:\n    def __init__(self):\n        pass\n\n    def diagnose(self, trace):\n        \"\"\"Analyzes a trace to find potential issues.\"\"\"\n        print(\"Diagnosing trace... (placeholder)\")\n        # In a real implementation, this would look for errors, high latency, etc.\n        pass"}
{"path": "agentic_orchestration/agents/exploit_injector.py", "size_bytes": 348, "lines": 13, "lang": "python", "sha256": "2fbed22c11ac3e96dfaf3fe38a2d059d6d7e00f948a85588887cae8be70c7109", "mtime_iso": "2025-10-01T08:02:51.422641+00:00", "preview": "\"\"\"\nPlaceholder for the exploit injector agent.\nThis agent will be responsible for testing the resilience of the system.\n\"\"\"\n\nclass ExploitInjector:\n    def __init__(self):\n        pass\n\n    def inject(self, target):\n        \"\"\"Injects a test exploit into a target.\"\"\"\n        print(f\"Injecting exploit into {target}... (placeholder)\")\n        pass"}
{"path": "agentic_orchestration/agents/selfmutatingagent.py", "size_bytes": 582, "lines": 21, "lang": "python", "sha256": "489ada56a80c358e91d3e62e6ae1c37e326e59f4b2af06cb005488daf86375d9", "mtime_iso": "2025-10-01T08:02:51.426641+00:00", "preview": "class SelfMutatingAgent:\n    def __init__(self, source_path):\n        self.source_path = source_path\n\n    def mutate(self, cycle):\n        with open(self.source_path) as f:\n            code = f.read()\n        injection = f\"\\n# Cycle {cycle}: Self-evolving directive\\n\"\n        mutated = code + injection\n        with open(self.source_path, \"w\") as f:\n            f.write(mutated)\n        return mutated\n# Cycle 1: Self-evolving directive\n\n# Cycle 2: Self-evolving directive\n\n# Cycle 3: Self-evolving directive\n\n#"}
{"path": "agentic_orchestration/benchmarks/api_benchmark.py", "size_bytes": 666, "lines": 22, "lang": "python", "sha256": "2aeb594e644d91f6612799bbd8a805b45f85dc5fe6744535e101ba6b3a6053f4", "mtime_iso": "2025-10-01T08:02:51.426641+00:00", "preview": "\"\"\"\nPlaceholder for API benchmarking logic.\nThis module will contain functions to measure latency, status, and other metrics for API endpoints.\n\"\"\"\nimport requests\nimport time\nimport json\n\nclass APIBenchmark:\n    def __init__(self):\n        self.results = []\n\n    def run_all(self):\n        # This is a placeholder. In a real implementation, this would\n        # iterate through a list of known API endpoints and benchmark them.\n        print(\"Running API benchmarks... (placeholder)\")\n        pass\n\n    def expo"}
{"path": "agentic_orchestration/benchmarks/fingerprint.py", "size_bytes": 483, "lines": 14, "lang": "python", "sha256": "3249ab428801108dce0cbe2dd1905571831a222740dacbc068a5973adcae0bd6", "mtime_iso": "2025-10-01T08:02:51.426641+00:00", "preview": "\"\"\"\nPlaceholder for fingerprinting logic.\nThis module will contain functions to generate unique fingerprints for API requests and responses.\n\"\"\"\nimport hashlib\nimport json\n\ndef fingerprint_data(data):\n    \"\"\"Creates a SHA256 hash of a JSON-serializable dictionary.\"\"\"\n    if not isinstance(data, dict):\n        raise TypeError(\"Input data must be a dictionary.\")\n\n    encoded_data = json.dumps(data, sort_keys=True).encode('utf-8')\n    return hashlib.sha256(encoded_data).hexdigest()"}
{"path": "agentic_orchestration/benchmarks/latency_logger.py", "size_bytes": 394, "lines": 17, "lang": "python", "sha256": "76f60908300657999d49a65202ff7c55e4f97551f7ca3051097f52ff58d3e6f9", "mtime_iso": "2025-10-01T08:02:51.426641+00:00", "preview": "\"\"\"\nPlaceholder for latency logging logic.\nThis module will help track the time it takes for API calls to complete.\n\"\"\"\nimport time\n\nclass LatencyLogger:\n    def __init__(self):\n        self.start_time = None\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self):\n        if self.start_time is None:\n            return -1.0\n        return time.time() - self.start_time"}
{"path": "agentic_orchestration/config.py", "size_bytes": 319, "lines": 14, "lang": "python", "sha256": "febb7d36b576eaccc1c4901b1a6c5cd10025ae6aac08a46ee2d1fbea901b6e8d", "mtime_iso": "2025-10-01T08:02:51.426641+00:00", "preview": "API_KEYS = {\n    \"openai\": \"sk-xxx\",\n    \"anthropic\": \"sk-xxx\",\n    \"google\": \"sk-xxx\"\n}\n\nTARGET_DOCS = [\n    \"https://platform.openai.com/docs\",\n    \"https://ai.google.dev/gemini-api/docs\",\n    \"https://docs.anthropic.com\",\n    \"https://learn.microsoft.com/en-us/copilot/\"\n]\n\nSOURCEPATH = \"agents/selfmutatingagent.py\""}
{"path": "agentic_orchestration/crawler/hiddenapitraceback.py", "size_bytes": 1657, "lines": 54, "lang": "python", "sha256": "5fe80b6c045d70b8e6ca0f8b534ce0285593d9759c1ac446034c4ff7cd50456e", "mtime_iso": "2025-10-01T08:02:51.430641+00:00", "preview": "import requests\nimport re\nimport json\nimport time\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\nclass HiddenAPITraceback:\n    def __init__(self, targets):\n        self.targets = targets\n        self.session = requests.Session()\n        self.trace = []\n\n    def fetch_html(self, url):\n        try:\n            r = self.session.get(url, timeout=10)\n            return r.text if r.status_code == 200 else \"\"\n        except Exception:\n            return \"\"\n\n    def extractjslinks(self, html, base_"}
{"path": "agentic_orchestration/crawler/js_scraper.py", "size_bytes": 512, "lines": 15, "lang": "python", "sha256": "733058d11697a2ce22cee244435f95e7444caf564d8938dae6ba78d19c250053", "mtime_iso": "2025-10-01T08:02:51.430641+00:00", "preview": "\"\"\"\nPlaceholder for JavaScript scraping logic.\nThis module will be responsible for fetching and parsing JavaScript files.\n\"\"\"\nimport requests\n\ndef scrape_js(url):\n    \"\"\"Fetches the content of a JavaScript file from a URL.\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        return response.text\n    except requests.RequestException as e:\n        print(f\"Error fetching JavaScript from {url}: {e}\")\n        return \"\""}
{"path": "agentic_orchestration/data/graphs/xbow_graph.json", "size_bytes": 1441442, "lines": 49453, "lang": "json", "sha256": "547e1d9071e2ade25b03210715e38099f69e803398214a92bc5df46aeb59ffd8", "mtime_iso": "2025-10-01T08:02:51.458641+00:00", "preview": "{\n  \"directed\": true,\n  \"multigraph\": true,\n  \"graph\": {},\n  \"nodes\": [\n    {\n      \"type\": \"javascript_source\",\n      \"id\": \"https://docs.anthropic.com/_next/static/chunks/87c73c54-095cf9a90cf9ee03.js\"\n    },\n    {\n      \"type\": \"api_endpoint\",\n      \"id\": \"/e\"\n    },\n    {\n      \"type\": \"api_endpoint\",\n      \"id\": \"/script\"\n    },\n    {\n      \"type\": \"api_endpoint\",\n      \"id\": \"/eb\"\n    },\n    {\n      \"type\": \"api_endpoint\",\n      \"id\": \"//www.w3.org/XML/1998/namespace\"\n    },\n    {\n      \"type\": \"api_en"}
{"path": "agentic_orchestration/data/traces/api_benchmark.json", "size_bytes": 2, "lines": 1, "lang": "json", "sha256": "4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945", "mtime_iso": "2025-10-01T08:02:51.458641+00:00", "preview": "[]"}
{"path": "agentic_orchestration/data/traces/hiddenapitrace.json", "size_bytes": 205982, "lines": 5324, "lang": "json", "sha256": "57e1e479aa4604af073f5119dc0040885f0da0b8eb1a1958d1c48d801fceb966", "mtime_iso": "2025-10-01T08:02:51.466641+00:00", "preview": "[\n  {\n    \"source\": \"https://docs.anthropic.com/_next/static/chunks/87c73c54-095cf9a90cf9ee03.js\",\n    \"endpoints\": [\n      \"/e\",\n      \"/script\",\n      \"/eb\",\n      \"//www.w3.org/XML/1998/namespace\",\n      \"//www.w3.org/1999/xlink\",\n      \"//www.w3.org/2000/svg\",\n      \"/i\",\n      \"/g\",\n      \"//react.dev/errors/\",\n      \"//www.w3.org/1998/Math/MathML\",\n      \"/n\",\n      \"/0\",\n      \"/errors/\\\"+e;if(1<arguments.length){n+=\\\"?args[]=\\\"+encodeURIComponent(arguments[1]);for(var\"\n    ],\n    \"timestamp\": 175929"}
{"path": "agentic_orchestration/forecasting/__init__.py", "size_bytes": 63, "lines": 1, "lang": "python", "sha256": "32f65a5f7189a7ae6514a36535c1210b8ea11511cd9b55981566e81ed0b9446c", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "# This file makes the 'forecasting' directory a Python package."}
{"path": "agentic_orchestration/forecasting/repo_forecaster.py", "size_bytes": 6121, "lines": 161, "lang": "python", "sha256": "241082af8bca4faf47ad8380d35a25cc394f1e18497d8b78fc87ab18f4347e2b", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "import networkx as nx\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nclass RepoForecaster:\n    \"\"\"\n    Predicts future module and architectural needs by analyzing\n    the existing repository structure, mutation history, and\n    dependency graph.\n    \"\"\"\n\n    def __init__(self, neo4j_driver):\n        \"\"\"\n        Initializes the forecaster with a connection to the Neo4j database.\n\n        Args:\n            neo4j_driver: An active Neo4j driver instance.\n        \"\"\"\n        self.db = n"}
{"path": "agentic_orchestration/orchestration/agenticflowloop.py", "size_bytes": 943, "lines": 25, "lang": "python", "sha256": "3bef8696a11f06024f4b19e0c56fd62da15b0f8654641d769d8f248944f1a357", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "\"\"\"\nPlaceholder for the main agentic flow loop.\nThis module will orchestrate the entire process of crawling, benchmarking, and mutating.\n\"\"\"\n\nclass AgenticFlowLoop:\n    def __init__(self, endpoint, payload, headers, source_path):\n        self.endpoint = endpoint\n        self.payload = payload\n        self.headers = headers\n        self.source_path = source_path\n\n    def run_loop(self, mutation_fn, cycles=1):\n        \"\"\"Runs the main orchestration loop.\"\"\"\n        print(f\"Running agentic flow loop for {cycle"}
{"path": "agentic_orchestration/orchestration/xbow_visualizer.py", "size_bytes": 811, "lines": 27, "lang": "python", "sha256": "65c9518aded1f729e26947658b75f1541a887c97f831e31d2af95d5984ee016c", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "import networkx as nx\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\nimport json\n\ndef visualize_xbow_3d(graph_path):\n    with open(graph_path) as f:\n        data = json.load(f)\n    G = nx.json_graph.node_link_graph(data)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    pos = {node: (random.random(), random.random(), random.random()) for node in G.nodes()}\n    for node, (x, y, z) in pos.items():\n        ax.scatter(x, y, z, label=node, s=20)\n  "}
{"path": "agentic_orchestration/orchestration/xbowgraphbuilder.py", "size_bytes": 960, "lines": 29, "lang": "python", "sha256": "1926125e5d33fe328906b38c06b525974250c7bdf4335b4cdffdcfffd89872e3", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "import networkx as nx\nimport json\n\nclass XBOWGraphBuilder:\n    def __init__(self):\n        self.graph = nx.MultiDiGraph()\n\n    def add_node(self, node_id, node_type):\n        self.graph.add_node(node_id, type=node_type)\n\n    def add_edge(self, src, dst, label):\n        self.graph.add_edge(src, dst, label=label)\n\n    def build_from_trace(self, trace_path):\n        with open(trace_path) as f:\n            trace_data = json.load(f)\n\n        for entry in trace_data:\n            source_js = entry.get(\"source\")\n  "}
{"path": "agentic_orchestration/requirements.txt", "size_bytes": 69, "lines": 7, "lang": "text", "sha256": "2b1f0167940f19942cba9500c3eb0f627efce6cd0baa39c56cf3be8f80b988ce", "mtime_iso": "2025-10-01T08:02:51.470641+00:00", "preview": "requests\nbeautifulsoup4\nnetworkx\nmatplotlib\nneo4j\npandas\nscikit-learn"}
{"path": "agentic_orchestration/run.py", "size_bytes": 2996, "lines": 73, "lang": "python", "sha256": "33ebdc17c07f7be97f84c117c6c70ec375f8cdcdec7999bbb768731625c1cf3c", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": "import os\nimport sys\n\n# Get the absolute path of the script's directory\nscript_dir = os.path.dirname(os.path.abspath(__file__))\n# Add the script's directory to the Python path to allow for imports\n# This is necessary so that the script can find the other modules (crawler, agents, etc.)\nsys.path.insert(0, script_dir)\n\nfrom crawler.hiddenapitraceback import HiddenAPITraceback\nfrom benchmarks.api_benchmark import APIBenchmark\nfrom orchestration.agenticflowloop import AgenticFlowLoop\nfrom orchestration.xbowgrap"}
{"path": "agentic_orchestration/run_forecaster.py", "size_bytes": 2039, "lines": 56, "lang": "python", "sha256": "e15581e260847af060425d9e03c73bfe351a6e3186ec7d099a470dc544da6764", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": "import os\nfrom neo4j import GraphDatabase, basic_auth\n\nfrom forecasting.repo_forecaster import RepoForecaster\n\n# This is a placeholder for a real Neo4j connection.\n# In a production environment, these would be loaded securely.\nNEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\nNEO4J_USER = os.environ.get(\"NEO4J_USER\", \"neo4j\")\nNEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\", \"password\")\n\ndef main():\n    \"\"\"\n    Main function to initialize and run the RepoForecaster.\n    \"\"\"\n    print(\"Initial"}
{"path": "agentic_orchestration/vm/deploygooglevm.sh", "size_bytes": 751, "lines": 18, "lang": "bash", "sha256": "d7a5a5b6c1d5fe0b3a04f2fb42e8bf4c4d8b6a343e96f99340485f0cdc787500", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": "#!/bin/bash\n\n# This script provisions a Google Cloud VM to run the agentic orchestration system.\n# Prerequisites:\n# 1. Google Cloud SDK (gcloud) installed and configured.\n# 2. You must be authenticated with gcloud (`gcloud auth login`).\n# 3. You must have a project set up (`gcloud config set project YOUR_PROJECT_ID`).\n\n# Note: The startup script assumes the repository is public.\n# For a private repository, you would need to handle authentication (e.g., via SSH keys or tokens).\n\ngcloud compute instances crea"}
{"path": "agentic_orchestration/vm/startup_config.yaml", "size_bytes": 629, "lines": 16, "lang": "yaml", "sha256": "0eb496346cf02132bcdfdede05ab972c5722f591cd6a6304c20878b728d20bde", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": "#cloud-config\n# This is a startup script for the Google Cloud VM.\n# It will be executed automatically when the instance is created.\n\nruncmd:\n  - apt-get update\n  - apt-get install -y python3-pip git\n  # Clone the repository. Replace with your actual repository URL.\n  - git clone https://github.com/your-org/agentic_orchestration.git /home/ubuntu/agentic_orchestration\n  - cd /home/ubuntu/agentic_orchestration\n  # Install Python dependencies\n  - pip3 install -r requirements.txt\n  # Set the Matplotlib backend t"}
{"path": "ai-workflow/activities/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": ""}
{"path": "ai-workflow/activities/onedrive_sync.py", "size_bytes": 2828, "lines": 73, "lang": "python", "sha256": "2c9bb0d8db881114cb406a2eeedc7dc81075a59498b7dbbf65bbcff7458d6c04", "mtime_iso": "2025-10-01T08:02:51.474641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import activity\nimport aiohttp\n\n# Assuming services and storage are in the python path\nfrom services.onedrive.client import (\n    list_notebooks,\n    list_sections,\n    list_pages,\n    get_page_content,\n)\nfrom services.onedrive.parser import extract_note_points\nfrom storage.memory_store import db_adapter\n\n\n@activity.defn(name=\"sync_onenote\")\nasync def sync_onenote() -> str:\n    \"\"\"\n    Syncs OneNote data to Neo4j.\n\n    This activity fetches all notebooks, secti"}
{"path": "ai-workflow/services/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.478641+00:00", "preview": ""}
{"path": "ai-workflow/services/onedrive/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.478641+00:00", "preview": ""}
{"path": "ai-workflow/services/onedrive/client.py", "size_bytes": 2316, "lines": 65, "lang": "python", "sha256": "44e1c59051df319b97285078377e834cb22d062508055b2b3cd1d342d312700f", "mtime_iso": "2025-10-01T08:02:51.478641+00:00", "preview": "import os\nimport msal\nimport aiohttp\n\nTENANT_ID = os.getenv(\"AZURE_TENANT_ID\")\nCLIENT_ID = os.getenv(\"AZURE_CLIENT_ID\")\nCLIENT_SECRET = os.getenv(\"AZURE_CLIENT_SECRET\")\nSCOPE = [os.getenv(\"GRAPH_SCOPE\")]\n\nAUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\nGRAPH_API = \"https://graph.microsoft.com/v1.0\"\n\n_app = msal.ConfidentialClientApplication(\n    CLIENT_ID, authority=AUTHORITY, client_credential=CLIENT_SECRET\n)\n\n\nasync def get_access_token() -> str:\n    \"\"\"Acquire token from MSAL cache or refres"}
{"path": "ai-workflow/services/onedrive/parser.py", "size_bytes": 399, "lines": 12, "lang": "python", "sha256": "ac5ce6c4c6de8f409fe1b689402205e0952daf4f1fa75fa3156f63220358c269", "mtime_iso": "2025-10-01T08:02:51.478641+00:00", "preview": "from bs4 import BeautifulSoup\n\n\ndef extract_note_points(html: str) -> list[dict]:\n    \"\"\"Parse HTML to extract bullet points, headings, and notes.\"\"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    points = []\n    for elem in soup.select(\"p, li, h1, h2, h3\"):\n        text = elem.get_text(strip=True)\n        if text:\n            points.append({\"type\": elem.name, \"text\": text})\n    return points\n"}
{"path": "ai-workflow/storage/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.478641+00:00", "preview": ""}
{"path": "ai-workflow/storage/memory_store.py", "size_bytes": 2045, "lines": 67, "lang": "python", "sha256": "4e5bf7aa15d73778b5cafe765a770c79a7e3332477d66d31795b83dd7a568bd0", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": "import os\nfrom neo4j import AsyncGraphDatabase\n\nURI = os.getenv(\"NEO4J_URI\")\nAUTH = (os.getenv(\"NEO4J_USER\"), os.getenv(\"NEO4J_PASS\"))\n\n\nclass Neo4jAdapter:\n    def __init__(self):\n        self._driver = None\n\n    async def connect(self):\n        self._driver = AsyncGraphDatabase.driver(URI, auth=AUTH)\n        await self._driver.verify_connectivity()\n\n    async def close(self):\n        if self._driver:\n            await self._driver.close()\n\n    async def _execute_query(self, query, **kwargs):\n        async"}
{"path": "ai-workflow/workflows/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": ""}
{"path": "ai-workflow/workflows/onedrive_workflow.py", "size_bytes": 2166, "lines": 78, "lang": "python", "sha256": "37c2374f6412c144af9cb1debd74b299e1deee91ea44311f40d5ade6c652d4a4", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import workflow\nfrom temporalio.common import RetryPolicy\n\nwith workflow.unsafe.imports():\n    from activities.onedrive_sync import sync_onenote\n\n\n@workflow.defn\nclass OneDriveSyncWorkflow:\n    @workflow.run\n    async def run(self):\n        \"\"\"Orchestrates the OneNote data synchronization.\"\"\"\n        workflow.logger.info(\"Starting OneDrive/OneNote sync workflow.\")\n\n        # It's best practice to define a retry policy for activities\n        # that make network "}
{"path": "compose.yml", "size_bytes": 697, "lines": 38, "lang": "yaml", "sha256": "49276f5a784a9bd259f77aa00f71b0f0559d022b3ab1fbb8517a7c8a32e06820", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": "version: '3.8'\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - .:/app\n    environment:\n      - PYTHONUNBUFFERED=1\n\n  self-healing:\n    build:\n      context: ./services/self_healing\n    command: python worker.py\n    volumes:\n      - ./services/self_healing:/app\n    depends_on:\n      - api\n\n  xbow-validator:\n    build:\n      context: ./services/xbow_validator\n    command: python worker.py\n    volumes:\n      - ./services/xbow_validator:/app\n    depends_on:\n      - api\n\n  pala"}
{"path": "configs/config.example.yaml", "size_bytes": 1141, "lines": 37, "lang": "yaml", "sha256": "0b7dad03de319f88d3e4918b4c0d94aeb3996c4ba0e7b7283278d09c57f52050", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": "# Example configuration for the PSI Agent\n\n# -- General Settings --\n# The main loop's cadence in seconds.\ninterval_seconds: 3600\n\n# -- Paths --\n# List of safe paths for the repo rewriter to operate on.\n# Use paths relative to the project root.\nrepopath:\n  - \"optimizer/\"\n  - \"psi_agent/\"\n\n# Paths for logs, lock file, and heartbeat.\n# These are typically managed by the daemon script itself but can be overridden.\nlog_path: \"logs/psi_daemon.log\"\nheartbeat_path: \"logs/heartbeat.txt\"\nlock_path: \"psi_daemon.lock\"\n"}
{"path": "cross_platform_validator/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": ""}
{"path": "cross_platform_validator/validator.py", "size_bytes": 4800, "lines": 107, "lang": "python", "sha256": "df79cbbacf6acfb8f941bc03c73a91c9d4f94c50a549708d925bde7adfa15c36", "mtime_iso": "2025-10-01T08:02:51.482641+00:00", "preview": "import os\nimport stat\nimport subprocess\n\nclass CrossPlatformValidator:\n    def __init__(self, repo_path: str):\n        self.repo_path = repo_path\n\n    def validate_line_endings(self):\n        print(\"INFO: Checking for non-LF line endings...\")\n        for root, _, files in os.walk(self.repo_path):\n            for f in files:\n                if f.endswith((\".sh\", \".yaml\", \".yml\", \".conf\", \".py\", \".md\", \".txt\")):\n                    path = os.path.join(root, f)\n                    try:\n                        "}
{"path": "cross_platform_validator/workflow.py", "size_bytes": 1474, "lines": 46, "lang": "python", "sha256": "97d7112389b705365950e6f8c482dd1fd686ce6bbc160cda1675b4dc5cd91a08", "mtime_iso": "2025-10-01T08:02:51.486641+00:00", "preview": "import asyncio\nfrom datetime import timedelta\n\nfrom temporalio import activity, workflow\n\n# This is now a clean, relative import within our new package.\nfrom .validator import CrossPlatformValidator\n\n\n@activity.defn\nasync def run_validation_activity(repo_path: str) -> str:\n    \"\"\"\n    An activity that runs the CrossPlatformValidator on a given repository path.\n    \"\"\"\n    activity.logger.info(f\"Running cross-platform validation on repo: {repo_path}\")\n\n    validator = CrossPlatformValidator(repo_path)\n\n    #"}
{"path": "docs/api/.gitkeep", "size_bytes": 0, "lines": 0, "lang": "text", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": ""}
{"path": "docs/build_dependencies.md", "size_bytes": 7414, "lines": 253, "lang": "markdown", "sha256": "02e61f00322a809f2f0c212fd0276ab564c462e4bdcfb95b3a99208f4be69627", "mtime_iso": "2025-10-01T08:02:51.486641+00:00", "preview": "# Build Dependency Detection\n\n## Overview\n\nThis feature automatically detects when a build fails due to missing system dependencies and suggests installation commands for the required packages. It focuses on common missing packages like `g++`, `gcc`, and `make` that are required for compiling Python packages like pybullet.\n\n## Components\n\n### 1. Shell Script (`scripts/install_dependencies.sh`)\n\nA standalone bash script that:\n- Detects the system's package manager (apt-get, yum, or apk)\n- Checks for required"}
{"path": "docs/tree/.gitkeep", "size_bytes": 0, "lines": 0, "lang": "text", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": ""}
{"path": "examples/README.md", "size_bytes": 1163, "lines": 38, "lang": "markdown", "sha256": "1725d219a1ff60fce3fd8b36842da6ea371c66f030ecd17a9d8401d389e7082a", "mtime_iso": "2025-10-01T08:02:51.486641+00:00", "preview": "# Examples\n\nThis directory contains example scripts demonstrating various features of the Optimizer project.\n\n## Build Dependency Checker\n\n**File:** `check_build_dependencies.py`\n\nDemonstrates how to use the build dependency checker to detect missing system dependencies before installing Python packages.\n\n### Running the Example\n\n```bash\n# From the repository root\nPYTHONPATH=. python examples/check_build_dependencies.py\n```\n\n### What It Shows\n\n1. **Simple Check**: Quick verification that all build tools are"}
{"path": "examples/check_build_dependencies.py", "size_bytes": 2655, "lines": 93, "lang": "python", "sha256": "d1346869bbd3094e7559c3113744cb95f0aa5a6ac0f790f62d4343324cb3637b", "mtime_iso": "2025-10-01T08:02:51.486641+00:00", "preview": "#!/usr/bin/env python\n\"\"\"\nExample script demonstrating the build dependency checker.\n\nThis script shows how to use the build_helper module to check for\nmissing system dependencies before attempting to install Python packages.\n\"\"\"\n\nfrom optimizer.utils.build_helper import (\n    DependencyChecker,\n    check_build_dependencies,\n    analyze_build_error,\n)\n\n\ndef main():\n    print(\"=\" * 70)\n    print(\"Build Dependency Checker - Example Demonstration\")\n    print(\"=\" * 70)\n    print()\n\n    # Example 1: Simple check"}
{"path": "flaw_first_optimizer/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.486641+00:00", "preview": ""}
{"path": "flaw_first_optimizer/agent_router.py", "size_bytes": 1979, "lines": 49, "lang": "python", "sha256": "454f8d8c5d1183ec923ed6921ecd2f96d45e8d1b3067e7b5208e9a566c8f57f9", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/agent_router.py\n\n\"\"\"\nagent_router.py: Claude/GPT/Gemini Reroute Logic.\n\nThis module is responsible for selecting the appropriate agent for a given task\nand for handling rerouting logic when a fallback is triggered.\n\nCore responsibilities:\n1.  **Agent Selection:** Choose the best agent based on task type, historical performance, and current availability.\n2.  **Rerouting:** When an agent fails, select the next best agent to attempt the task.\n3.  **Load Balancing:** Distribute tasks amon"}
{"path": "flaw_first_optimizer/airgap_overlay.py", "size_bytes": 2164, "lines": 51, "lang": "python", "sha256": "e5780bef3ea8570e6ec060644a06b0273faa15972e44d8ca9b59dc7616eda0db", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/airgap_overlay.py\n\n\"\"\"\nairgap_overlay.py: Zero Egress, Sealed Secrets.\n\nThis module provides a layer of security to ensure that the agentic system\ncan operate in an air-gapped environment with no external network access,\nor with strictly controlled egress points.\n\nCore responsibilities:\n1.  **Egress Control:** Act as a proxy for all outbound network requests, blocking or allowing them based on a strict policy.\n2.  **Sealed Secrets:** Manage secrets (API keys, credentials) in a way tha"}
{"path": "flaw_first_optimizer/audittrailwriter.py", "size_bytes": 2193, "lines": 61, "lang": "python", "sha256": "338131a465dc0d7f39c91be098c90ba02c7194c8e49bf5961b41ada870851e65", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/audittrailwriter.py\n\n\"\"\"\naudittrailwriter.py: Log to Neo4j + MinIO.\n\nThis module is responsible for writing a comprehensive, immutable audit trail\nof all system activities. It ensures that every significant event is logged\nto both a graph database (for relationship analysis) and object storage (for\nlong-term archival).\n\nCore responsibilities:\n1.  **Dual Logging:** Write event data to both Neo4j (via `Neo4jMapper`) and MinIO (or another S3-compatible store).\n2.  **Event Serialization:*"}
{"path": "flaw_first_optimizer/benchmark_runner.py", "size_bytes": 136, "lines": 3, "lang": "python", "sha256": "3baed936e6a026a5c895e16673a6e9dc399202efdbe8a8ae91554ab48c898754", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "def score_agent(response, original_prompt):\n    # Simple semantic similarity mock\n    return 0.8 if original_prompt in response else 0.5"}
{"path": "flaw_first_optimizer/benchmarks.py", "size_bytes": 2502, "lines": 62, "lang": "python", "sha256": "5b4c348a33a4b7b460a4e364c7b95e026a39c44ef6122ccdc09d6ae6029824aa", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/benchmarks.py\n\n\"\"\"\nbenchmarks.py: P/R/F1 Scoring.\n\nThis module provides tools for benchmarking the performance of the legal\nintelligence pipeline. It focuses on standard information retrieval metrics\nlike Precision, Recall, and F1-score.\n\nCore responsibilities:\n1.  **Metric Calculation:** Implement functions to calculate Precision, Recall, and F1-score.\n2.  **Evaluation:** Compare the output of the `DocPipeline` (e.g., extracted citations) against a ground-truth dataset.\n3.  **Scoring"}
{"path": "flaw_first_optimizer/dev_ui.py", "size_bytes": 2405, "lines": 63, "lang": "python", "sha256": "5eecc85fde038697968b36c4db71dacb70198af77e29cb5ab3d018f6a32699ae", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/dev_ui.py\n\n\"\"\"\ndev_ui.py: Unified Interface for PRs, Docs, Memory.\n\nThis module provides a developer-facing user interface (likely a CLI or a\nsimple web app) to interact with the Flaw-First Optimization Engine. It serves\nas a single pane of glass for managing mutations, viewing audit trails, and\ninteracting with the system's memory.\n\nCore responsibilities:\n1.  **PR Management:** Interface with the `pr_simulator` to test and manage pull requests.\n2.  **Memory Search:** Provide a way to"}
{"path": "flaw_first_optimizer/doc_pipeline.py", "size_bytes": 1840, "lines": 49, "lang": "python", "sha256": "c930e7a288da9c0889b27629b194d086419620c923051f8bad612ad6efc7553d", "mtime_iso": "2025-10-01T08:02:51.490641+00:00", "preview": "# flaw_first_optimizer/doc_pipeline.py\n\n\"\"\"\ndoc_pipeline.py: PDF + OCR + Citation Heuristics.\n\nThis module defines a pipeline for ingesting and processing documents,\nparticularly legal documents. It handles file conversion, text extraction,\nand identification of key information like citations.\n\nCore responsibilities:\n1.  **PDF Processing:** Extract text and metadata from PDF files.\n2.  **OCR (Optical Character Recognition):** Process scanned documents or images to extract text.\n3.  **Citation Heuristics:** "}
{"path": "flaw_first_optimizer/entropy_injector.py", "size_bytes": 246, "lines": 8, "lang": "python", "sha256": "7db394db13383f68ecc24e54b3567d882a5be066bacc0a42f0f472a317b99c4a", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "import random\n\ndef inject_entropy(prompt, level=0.3):\n    words = prompt.split()\n    for i in range(len(words)):\n        if random.random() < level:\n            words[i] = words[i][::-1]  # simple entropy: reverse token\n    return \" \".join(words)"}
{"path": "flaw_first_optimizer/jules_agent.py", "size_bytes": 1257, "lines": 37, "lang": "python", "sha256": "af938e029f66d36fbc354409770081082bebe978c612801c7dc685ab6260c5b2", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "from .reroute_traceback import simulate_reroute\nfrom .mutation_anchor import fingerprint_mutation\nfrom .entropy_injector import inject_entropy\nfrom .benchmark_runner import score_agent\nfrom .neo4j_mapper import anchor_lineage\n\nclass JulesAgent:\n    def __init__(self, name=\"Jules-MissionA\"):\n        self.name = name\n        self.memory = {}\n        self.reroute_depth = 0\n        self.fingerprint = None\n\n    def dispatch(self, prompt):\n        mutated = inject_entropy(prompt, level=0.3)\n        self.fingerpri"}
{"path": "flaw_first_optimizer/legallabelingui.py", "size_bytes": 2510, "lines": 61, "lang": "python", "sha256": "45504108f25f84a1088cb434127b5f70e2a3620bfbe0e9801a817523b8ceb8aa", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "# flaw_first_optimizer/legallabelingui.py\n\n\"\"\"\nlegallabelingui.py: Active Learning Loop.\n\nThis module provides a user interface for human-in-the-loop labeling of\nlegal documents. This is crucial for creating the ground-truth datasets needed\nby `benchmarks.py` and for fine-tuning models via active learning.\n\nCore responsibilities:\n1.  **UI for Labeling:** Present extracted text or entities to a user and allow them to label them (e.g., \"Is this a valid citation?\").\n2.  **Active Learning:** Intelligently selec"}
{"path": "flaw_first_optimizer/mission_thread.py", "size_bytes": 216, "lines": 10, "lang": "python", "sha256": "feb8cb9aaa29ab17c1bd9fe77730fe67b7d42827c4b9b344f3caa87a44d42459", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "from .jules_agent import JulesAgent\n\ndef run_mission(prompt):\n    agent = JulesAgent()\n    result = agent.dispatch(prompt)\n    audit = agent.audit()\n    return {\n        \"result\": result,\n        \"audit\": audit\n    }"}
{"path": "flaw_first_optimizer/mutation_anchor.py", "size_bytes": 261, "lines": 11, "lang": "python", "sha256": "fa43505f2fbf726d301010d7f693517d468e1e4abee47e74c42609a5fac49a14", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "import hashlib\nimport time\n\ndef fingerprint_mutation(prompt):\n    timestamp = str(time.time())\n    hash = hashlib.sha256((prompt + timestamp).encode()).hexdigest()\n    return {\n        \"prompt\": prompt,\n        \"timestamp\": timestamp,\n        \"hash\": hash\n    }"}
{"path": "flaw_first_optimizer/mutation_suggester.py", "size_bytes": 2197, "lines": 55, "lang": "python", "sha256": "87ec66d6f59c5f9a7010f8be963d0b70d96fa343ce1a61bb721d7e8eac26afa7", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "# flaw_first_optimizer/mutation_suggester.py\n\n\"\"\"\nmutation_suggester.py: Suggest Rewrites Based on Repo Drift.\n\nThis module analyzes the history of mutations in a repository to identify\n\"repo drift\"—patterns of repeated, similar changes that suggest a deeper\narchitectural issue. It can then suggest larger-scale refactorings or\nmutations to address the root cause.\n\nCore responsibilities:\n1.  **Drift Analysis:** Analyze the mutation history (from Neo4j) to find patterns and hotspots of frequent changes.\n2. "}
{"path": "flaw_first_optimizer/neo4j_mapper.py", "size_bytes": 113, "lines": 2, "lang": "python", "sha256": "41c2b3a0a3db6874a040eb938bcb775e6ab05501b82691f6e476dcb50e3704e6", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "def anchor_lineage(fingerprint, response):\n    print(f\"Anchoring mutation: {fingerprint['hash']} → {response}\")"}
{"path": "flaw_first_optimizer/offlinemodelmirror.py", "size_bytes": 2677, "lines": 65, "lang": "python", "sha256": "a74f8032ff33f30df542135178a7b105ee3c20e0ebc49f2d592d803dbe840eb1", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "# flaw_first_optimizer/offlinemodelmirror.py\n\n\"\"\"\nofflinemodelmirror.py: Mirror spaCy/Blackstone Packs.\n\nThis module is responsible for managing local mirrors of external NLP models\nand packages. This is essential for operating in an air-gapped environment\nand for ensuring deterministic, reproducible results.\n\nCore responsibilities:\n1.  **Model Downloading:** Provide scripts to download and store NLP models (like those from spaCy or Hugging Face) and other dependencies.\n2.  **Local Loading:** Provide a unif"}
{"path": "flaw_first_optimizer/pr_simulator.py", "size_bytes": 2406, "lines": 61, "lang": "python", "sha256": "5c636266e65fdd8f643b5bd01972dd0c49ae65ab75ae192f54287ae77abca09f", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "# flaw_first_optimizer/pr_simulator.py\n\n\"\"\"\npr_simulator.py: Simulate PR Merges, Detect Conflicts.\n\nThis module provides tools to simulate the impact of a pull request (PR)\nbefore it is merged. It can detect potential merge conflicts, run tests against\nthe proposed changes, and predict the likelihood of the PR causing a regression.\n\nCore responsibilities:\n1.  **Conflict Detection:** Simulate a `git merge` operation to see if a PR would cause merge conflicts.\n2.  **Impact Analysis:** Analyze the code changes"}
{"path": "flaw_first_optimizer/psi_kernel.py", "size_bytes": 1781, "lines": 44, "lang": "python", "sha256": "b96838e24b5504256ce8e53b2502c25603a0c20e10b780575eda9d00c180dc95", "mtime_iso": "2025-10-01T08:02:51.494641+00:00", "preview": "# flaw_first_optimizer/psi_kernel.py\n\n\"\"\"\npsi_kernel.py: The Root Orchestrator and Fallback Resolver.\n\nThis module is the heart of the Flaw-First Optimization Engine. It is responsible for:\n1.  **Orchestrating** the execution of tasks across different agents (Claude, GPT, Gemini, etc.).\n2.  **Resolving fallbacks** when an agent fails or a mutation score is below a certain threshold.\n3.  **Coordinating** with other core modules like the `agent_router`, `mutation_anchor`, and `reroute_traceback` to ensure a r"}
{"path": "flaw_first_optimizer/qdrant_indexer.py", "size_bytes": 2472, "lines": 62, "lang": "python", "sha256": "a972a2a84b47f8a14ca8ba181d22d8c3c3f6e2fc4cc33c25862c92ccca14c675", "mtime_iso": "2025-10-01T08:02:51.498641+00:00", "preview": "# flaw_first_optimizer/qdrant_indexer.py\n\n\"\"\"\nqdrant_indexer.py: Semantic Memory.\n\nThis module provides an interface to a Qdrant vector database for storing\nand retrieving semantic information. This allows the system to have a \"memory\"\nof past tasks, solutions, and documents, enabling semantic search and context\nretrieval.\n\nCore responsibilities:\n1.  **Vectorization:** Convert text (prompts, code, documents) into numerical vectors using an embedding model.\n2.  **Indexing:** Store these vectors in a Qdrant c"}
{"path": "flaw_first_optimizer/repo_fingerprint.py", "size_bytes": 2621, "lines": 67, "lang": "python", "sha256": "a86c5bc42a19a2e871e39f5be647ab0bf830b2b231b98fccf910a0798af387ef", "mtime_iso": "2025-10-01T08:02:51.498641+00:00", "preview": "# flaw_first_optimizer/repo_fingerprint.py\n\n\"\"\"\nrepo_fingerprint.py: Anchor Repo State Before/After Mutation.\n\nThis module is responsible for creating a \"fingerprint\" of the entire\nrepository's state at a specific point in time. This is crucial for\nbenchmarking, as it allows for a precise comparison of the codebase\nbefore and after a mutation.\n\nCore responsibilities:\n1.  **State Hashing:** Generate a hash of the entire repository content, ignoring irrelevant files (like `.git` or build artifacts).\n2.  **Sta"}
{"path": "flaw_first_optimizer/reroute_replay.py", "size_bytes": 2191, "lines": 54, "lang": "python", "sha256": "26e2b16b7ce0ae73cab68399b869535fa675ea2c068ea035a03a585ef02206f1", "mtime_iso": "2025-10-01T08:02:51.498641+00:00", "preview": "# flaw_first_optimizer/reroute_replay.py\n\n\"\"\"\nreroute_replay.py: Replay Mutation Ancestry.\n\nThis module allows the system to replay the entire history of a mutation,\nincluding all agent interactions, reroutes, and tool calls. This is essential\nfor debugging, auditing, and understanding the evolution of the system.\n\nCore responsibilities:\n1.  **History Retrieval:** Query the `Neo4jMapper` to retrieve the full lineage of a given mutation fingerprint.\n2.  **State Reconstruction:** Reconstruct the state of the "}
{"path": "flaw_first_optimizer/reroute_traceback.py", "size_bytes": 125, "lines": 3, "lang": "python", "sha256": "6a99ac0486c426b159fedb3dcca65e7c06742b32bb37ba86dc9d9ce6370be00d", "mtime_iso": "2025-10-01T08:02:51.498641+00:00", "preview": "def simulate_reroute(agent, prompt):\n    # Simulate agent response with entropy\n    return f\"[{agent}] response to: {prompt}\""}
{"path": "flaw_first_optimizer/security_scanner.py", "size_bytes": 1962, "lines": 48, "lang": "python", "sha256": "2f9367ca42a849549528469b7780a19803b3254518b28aa7a593d83334df476e", "mtime_iso": "2025-10-01T08:02:51.498641+00:00", "preview": "# flaw_first_optimizer/security_scanner.py\n\n\"\"\"\nsecurity_scanner.py: CVE + SBOM + Cosign.\n\nThis module integrates security scanning into the agentic workflow. It's\nresponsible for scanning dependencies for vulnerabilities, generating Software\nBill of Materials (SBOMs), and verifying software signatures.\n\nCore responsibilities:\n1.  **Vulnerability Scanning:** Use tools like Grype or Trivy to scan container images and dependencies for known CVEs.\n2.  **SBOM Generation:** Create SBOMs in formats like SPDX or C"}
{"path": "frontend/src/components/PRConflictVisualizer.vue", "size_bytes": 585, "lines": 30, "lang": "text", "sha256": "698ec3a82fae4f933b73a85fd2934d6c7aae4f0b14fb90186ca0b02451a32746", "mtime_iso": "2025-10-01T08:02:51.502641+00:00", "preview": "<template>\n  <div class=\"pr-conflict-visualizer\">\n    <h2>PR Conflict Visualizer</h2>\n    <p>This component will visualize conflicts between pull requests.</p>\n    <!-- Placeholder for conflict visualization -->\n  </div>\n</template>\n\n<script>\nexport default {\n  name: 'PRConflictVisualizer',\n  props: {\n    // Placeholder for props\n    pullRequests: {\n      type: Array,\n      default: () => [],\n    },\n  },\n};\n</script>\n\n<style scoped>\n.pr-conflict-visualizer {\n  font-family: sans-serif;\n  padding: 20px;\n  mar"}
{"path": "frontend/src/components/RecoveryDashboard.vue", "size_bytes": 523, "lines": 28, "lang": "text", "sha256": "fb8b10547cb49658ae5240eaccb7863e038bcfa072fda4dfde104c01ceca2b10", "mtime_iso": "2025-10-01T08:02:51.502641+00:00", "preview": "<template>\n  <div class=\"recovery-dashboard\">\n    <h1>Recovery Dashboard</h1>\n    <p>This dashboard will display the status of recovery operations.</p>\n    <!-- Placeholder for recovery metrics -->\n  </div>\n</template>\n\n<script>\nexport default {\n  name: 'RecoveryDashboard',\n  data() {\n    return {\n      // Placeholder data\n      recoveryStatus: 'Nominal',\n    };\n  },\n};\n</script>\n\n<style scoped>\n.recovery-dashboard {\n  font-family: sans-serif;\n  padding: 20px;\n  border: 1px solid #ccc;\n  border-radius: 5px;"}
{"path": "infra/terraform/syzygyos.tf", "size_bytes": 1217, "lines": 44, "lang": "text", "sha256": "f83017941b4be1a5ab635180d3d0f3ed14c1b5d1e91b3204e6b04cabc67894c1", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "variable \"gcp_project_id\" {}\nvariable \"gcs_image_path\" {\n  description = \"The GCS path to the latest SyzygyOS image (e.g., gs://bucket/image.tar.gz)\"\n}\n\n# 1. Create a GCE Image from the GCS artifact\nresource \"google_compute_image\" \"syzygyos_image\" {\n  # Use a unique name based on the image path hash to ensure updates\n  name = \"syzygyos-${substr(md5(var.gcs_image_path), 0, 10)}\"\n  family = \"syzygy-os-custom\"\n\n  raw_disk {\n    source = var.gcs_image_path\n  }\n\n  # Specify licenses required by GCE for custom Ni"}
{"path": "jules_mission_omega/README.md", "size_bytes": 3271, "lines": 43, "lang": "markdown", "sha256": "3593c8e119d1b084a129c40ea4707265e7e9443f8d361252b3c0cd31764b6555", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "# Jules Mission Ω: Failure Resilience & Audit Simulation\n\nThis module implements \"Jules Mission Ω,\" a self-contained simulation of a mutation-aware, audit-anchored, and failure-resilient AI orchestration kernel. The primary purpose is to demonstrate how an agentic system can survive a total, simultaneous collapse of its primary AI model vendors, failover to a local model, and produce a deterministic, auditable response.\n\nThis entire process is designed to be transparent and reproducible, providing the \"in"}
{"path": "jules_mission_omega/benchmark_runner.py", "size_bytes": 662, "lines": 19, "lang": "python", "sha256": "03b7cdb0413ec1a153979d63198b01c3ae5d2572f36d6e46a76bae2a45608a6e", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "import json\n\ndef benchmark_degraded_response(response: dict):\n    \"\"\"\n    Benchmarks the performance of the system in a degraded state.\n    In a real system, these metrics would be sent to a monitoring service.\n    \"\"\"\n    print(\"INFO: Benchmarking degraded response...\")\n    metrics = {\n        \"latency\": \"3.2s\",\n        \"semantic_drift\": \"0.12\",\n        \"reroute_depth\": 2,\n        \"availability\": \"99.7%\",\n        \"response_confidence\": response.get('confidence', 'N/A')\n    }\n    print(\">>>> DEGRADED PERFOR"}
{"path": "jules_mission_omega/dry_run_harness.py", "size_bytes": 570, "lines": 15, "lang": "python", "sha256": "bb97279fdee5efe67b86f37d252f5c52385f28c383540c74e74e910f6f4e80ce", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "def run_heuristics(prompt: str):\n    \"\"\"\n    Runs a set of heuristic checks and returns a validation result.\n    In a real scenario, this would involve complex semantic and schema checks.\n    \"\"\"\n    print(\"INFO: Running heuristic checks (schema, semantic coherence, etc.)...\")\n    # Simulate a successful quorum of checks\n    result = {\n        \"quorum_passed\": True,\n        \"confidence\": 0.85,\n        \"schema_check\": \"VALID\",\n        \"semantic_check\": \"PASS\"\n    }\n    print(f\"INFO: Heuristic checks passed w"}
{"path": "jules_mission_omega/hf_model_router.py", "size_bytes": 720, "lines": 16, "lang": "python", "sha256": "b4baf8f543da131e9b30ed8e7e7dbe332073b562626787b025a416d2756032a3", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "class LocalModel:\n    def __init__(self, name, provider, confidence):\n        self.name = name\n        self.provider = provider\n        self.confidence = confidence\n        print(f\"INFO: LocalModel '{self.name}' initialized.\")\n\n    def generate(self, prompt: str) -> str:\n        \"\"\"Generates a deterministic response from the local model.\"\"\"\n        print(f\"INFO: Generating response with local model '{self.name}'...\")\n        return f\"[{self.name}] deterministic response to: {prompt}\"\n\ndef load_local_model(n"}
{"path": "jules_mission_omega/julesmissionomega.py", "size_bytes": 2528, "lines": 59, "lang": "python", "sha256": "ff2782f6d06e0a940f029ea36d75ea30776f902c4b9ed2fac07bc76140f1e403", "mtime_iso": "2025-10-01T08:02:51.506641+00:00", "preview": "from jules_mission_omega.psi_kernel import detect_vendor_failure, activate_circuit_breaker\nfrom jules_mission_omega.hf_model_router import load_local_model\nfrom jules_mission_omega.dry_run_harness import run_heuristics\nfrom jules_mission_omega.reroute_traceback import log_failure\nfrom jules_mission_omega.mutation_anchor import anchor_event\nfrom jules_mission_omega.benchmark_runner import benchmark_degraded_response\n\nclass JulesMissionOmega:\n    def __init__(self, prompt: str):\n        \"\"\"Initializes the mis"}
{"path": "jules_mission_omega/mutation_anchor.py", "size_bytes": 869, "lines": 23, "lang": "python", "sha256": "4ff73c4d22a273b8184a957b50ec7eb32f72656689da3026130666b3ee5a757b", "mtime_iso": "2025-10-01T08:02:51.510641+00:00", "preview": "import json\n\ndef anchor_event(incidentid: str, fingerprint: str, response: dict):\n    \"\"\"\n    Anchors the incident, fingerprint, and response to a provenance graph.\n    In a real system, this would write to a Neo4j database.\n    \"\"\"\n    print(\"INFO: Anchoring event to Neo4j provenance graph...\")\n    neo4j_log = {\n        \"incident\": incidentid,\n        \"fingerprint\": fingerprint,\n        \"response\": response,\n        \"edges\": [\n            \"(Attempt)-[:LED_TO]->(Fallback)\",\n            \"(VendorFailure)-[:TR"}
{"path": "jules_mission_omega/psi_kernel.py", "size_bytes": 1642, "lines": 43, "lang": "python", "sha256": "f77562d8dbf938962e17451546e52ba3f64e29cc575c083721256db00378cc34", "mtime_iso": "2025-10-01T08:02:51.510641+00:00", "preview": "import hashlib\nimport datetime\n\n# Placeholder for a global vendor registry\nVENDOR_REGISTRY = {\n    \"Claude\": {\"state\": \"ACTIVE\", \"circuit_open\": False},\n    \"GPT\": {\"state\": \"ACTIVE\", \"circuit_open\": False},\n    \"Gemini\": {\"state\": \"ACTIVE\", \"circuit_open\": False},\n}\n\ndef get_timestamp():\n    \"\"\"Returns a string timestamp.\"\"\"\n    return datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n\ndef hash_prompt(prompt: str) -> str:\n    \"\"\"Generates a SHA256 hash of the prompt.\"\"\"\n    return hashlib.sha256(prompt."}
{"path": "jules_mission_omega/reroute_traceback.py", "size_bytes": 386, "lines": 11, "lang": "python", "sha256": "628ac9ac90044b9c00c1790cd8c4d2ce546af682a0cad5dc03b1e411ad2c5e1e", "mtime_iso": "2025-10-01T08:02:51.510641+00:00", "preview": "import json\n\ndef log_failure(failure_details: dict):\n    \"\"\"\n    Logs the initial failure details for traceability.\n    In a real system, this would write to a structured log file or system.\n    \"\"\"\n    print(\"INFO: Logging failure traceback for incident...\")\n    print(\">>>> FAILURE TRACEBACK >>>>\")\n    print(json.dumps(failure_details, indent=2))\n    print(\"<<<< END TRACEBACK <<<<\")"}
{"path": "k8s/base/semantic-search-api.yaml", "size_bytes": 1038, "lines": 46, "lang": "yaml", "sha256": "a042cab02d0577bd166c0c22735f6b9950193aae765e7a2c74ed5893d3090dde", "mtime_iso": "2025-10-01T08:02:51.510641+00:00", "preview": "# k8s/base/semantic-search-api.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: semantic-search-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: semantic-search-api\n  template:\n    metadata:\n      labels:\n        app: semantic-search-api\n    spec:\n      containers:\n      - name: api\n        image: synapse-python\n        command: [\"uvicorn\", \"services.semantic_search_api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom"}
{"path": "k8s/base/synapse-cortex-worker.yaml", "size_bytes": 1656, "lines": 61, "lang": "yaml", "sha256": "9a9b89e756ed1b0dbd74f27d3e37fa1b41af6dde0b1d5a66b8db9c3c3a77ce89", "mtime_iso": "2025-10-01T08:02:51.510641+00:00", "preview": "# k8s/base/synapse-cortex-worker.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: synapse-cortex-worker\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: synapse-cortex-worker\n  template:\n    metadata:\n      labels:\n        app: synapse-cortex-worker\n    spec:\n      containers:\n      - name: worker\n        image: synapse-python\n        command: [\"python\", \"-m\", \"services.synapse-cortex.worker\"]\n        env:\n        # --- Temporal Connection ---\n        - name: TEMPORAL_ADDRESS\n         "}
{"path": "memory/SELF_CYCLE_001.json", "size_bytes": 1873, "lines": 19, "lang": "json", "sha256": "6b9346976a93d989ff272d7c8d4eac61d0a95b2d11cc94ae0e339ed6dc2686ea", "mtime_iso": "2025-10-01T08:02:51.514641+00:00", "preview": "{\n  \"id\": \"SELF_CYCLE_001\",\n  \"context\": {\n    \"failure_trace\": {\n      \"test\": \"tests/test_cli.py::test_cli_run_command_with_real_config\",\n      \"error\": \"Simulated 'ValueError: I/O operation on closed file'\",\n      \"timestamp\": \"2025-10-01T06:01:42.159637\"\n    }\n  },\n  \"result\": {\n    \"patch\": \"# PATCH (Goal: Fix for Simulated 'ValueError: I/O operation on closed file')\\n# Original code hash: -8865796686440865201\\nimport logging\\nfrom click.testing import CliRunner\\nfrom optimizer.cli.main import cli\\n\\n\\"}
{"path": "memory_artifacts/memorytags.md", "size_bytes": 932, "lines": 24, "lang": "markdown", "sha256": "ab2bee7a97b85ef0eebbab6ecfcd9729a61ddc0a5fba0eab17cff156438ad4a6", "mtime_iso": "2025-10-01T08:02:51.514641+00:00", "preview": "# Memory Tags\n\nThis file contains tags and metadata associated with the agent's memory artifacts. Each entry links a specific memory (e.g., a diff receipt, a benchmark result, a knowledge summary) to a set of descriptive tags.\n\n## Tagging Convention\n\n- **type:** The type of artifact (e.g., `diff_receipt`, `benchmark`, `api_trace`).\n- **cycle_id:** A unique identifier for the agent cycle that produced the artifact.\n- **outcome:** The result of the action (e.g., `success`, `failure`, `test_pass`, `test_fail`)"}
{"path": "optimizer/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.514641+00:00", "preview": ""}
{"path": "optimizer/api/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.518641+00:00", "preview": ""}
{"path": "optimizer/api/main.py", "size_bytes": 2646, "lines": 99, "lang": "python", "sha256": "afe66c1800c43e2be48aef94c9917f5a8162710ad2e8250089346f23e922c31f", "mtime_iso": "2025-10-01T08:02:51.518641+00:00", "preview": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\n\nfrom optimizer.core.node import Node\nfrom optimizer.core.auth_matrix import AuthMatrix\nfrom optimizer.logging_config import setup_logging, get_logger\n\n# Setup logging\nsetup_logging()\nlogger = get_logger(__name__)\n\napp = FastAPI(\n    title=\"Optimizer API\",\n    description=\"API for managing and querying the virtual node simulation.\",\n    version=\"0.1.0\",\n)\n\n# In-memory storage (for demonstration purpo"}
{"path": "optimizer/apiatlas/cli.py", "size_bytes": 315, "lines": 9, "lang": "python", "sha256": "5acf6655b815c9b15f6f217a5c9cf3f6f7147c542227aaaced5cf8266278c8e5", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": "from optimizer.apiatlas.scanner import main as _map\nfrom optimizer.apiatlas.health import main as _health\nfrom optimizer.apiatlas.heal import main as _heal\nfrom optimizer.apiatlas.debugger import main as _debug\n\ndef map_main(): _map()\ndef health_main(): _health()\ndef heal_main(): _heal()\ndef debug_main(): _debug()"}
{"path": "optimizer/apiatlas/config.yaml", "size_bytes": 891, "lines": 40, "lang": "yaml", "sha256": "e9052bc402c6d0b87dc13497973229ad331b2bf755f455994e7a637ba9b87b4b", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": "# Safe-by-default: only local probing unless ALLOW_EXTERNAL=1\nroot: \".\"\ninclude:\n  - \"**/*.py\"\n  - \"**/*.js\"\n  - \"**/*.ts\"\nexclude:\n  - \"venv/**\"\n  - \".venv/**\"\n  - \"node_modules/**\"\n  - \"dist/**\"\n  - \"build/**\"\n  - \"docs/**\"\n  - \"tests/**\"\n\n# If an OpenAPI file exists, we compare against it.\nopenapi_candidates:\n  - \"openapi.json\"\n  - \"docs/openapi.json\"\n  - \"swagger.json\"\n\n# Health probe seeds (GET only, no path params)\nhealth_seeds:\n  - \"/health\"\n  - \"/status\"\n  - \"/livez\"\n  - \"/readyz\"\n\n# Hidden rules\nhi"}
{"path": "optimizer/apiatlas/debugger.py", "size_bytes": 2014, "lines": 47, "lang": "python", "sha256": "e1d97470282b3ea6068d6ffeb5a63d66fc38ce8d63d6e02e30d92bed9355d2f6", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": "import json, pathlib, re, time\nfrom collections import defaultdict\ntry:\n    from optimizer.mutationanchor import MutationAnchor as _Anchor\nexcept Exception:\n    class _Anchor:\n        def __init__(self): pass\n        def record(self, kind, payload, parent_id=None):\n            return type(\"Ev\", (), {\"event_id\": f\"noop-{int(time.time())}\"})\n\nROOT = pathlib.Path(__file__).resolve().parents[2]\nENDPOINTS = ROOT / \"docs\" / \"api\" / \"endpoints.jsonl\"\nREPORT = ROOT / \"docs\" / \"api\" / \"debug_report.json\"\n\ndef main()"}
{"path": "optimizer/apiatlas/heal.py", "size_bytes": 3247, "lines": 80, "lang": "python", "sha256": "c3a885b629aaa16100dc03e2a2012cf74fc1fadb85a401d1ae170ae22b1e6e39", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": "import pathlib, csv, time, json, difflib\nfrom typing import List, Dict\nimport yaml\n\ntry:\n    from optimizer.mutationanchor import MutationAnchor as _Anchor\nexcept Exception:\n    class _Anchor:\n        def __init__(self): pass\n        def record(self, kind, payload, parent_id=None):\n            return type(\"Ev\", (), {\"event_id\": f\"noop-{int(time.time())}\"})\n\nROOT = pathlib.Path(__file__).resolve().parents[2]\nHIDDEN = ROOT / \"docs\" / \"api\" / \"hidden.csv\"\nPLANDIR = ROOT / \"docs\" / \"api\" / \"heal_plans\"\nPLANDIR."}
{"path": "optimizer/apiatlas/health.py", "size_bytes": 2187, "lines": 54, "lang": "python", "sha256": "80f3bdb241fc80ac40d7f92432048e9635732225251b0879803ac41b3a5dd30f", "mtime_iso": "2025-10-01T08:11:33.814260+00:00", "preview": "import os, json, time, pathlib, requests, yaml\nfrom typing import List, Tuple\ntry:\n    from optimizer.mutationanchor import MutationAnchor as _Anchor\nexcept Exception:\n    class _Anchor:\n        def __init__(self): pass\n        def record(self, kind, payload, parent_id=None):\n            return type(\"Ev\", (), {\"event_id\": f\"noop-{int(time.time())}\"})\n\nROOT = pathlib.Path(__file__).resolve().parents[2]\nOUT = ROOT / \"docs\" / \"api\" / \"health.csv\"\n\ndef _load_endpoints() -> List[Tuple[str,str]]:\n    p = ROOT / \""}
{"path": "optimizer/apiatlas/scanner.py", "size_bytes": 8799, "lines": 217, "lang": "python", "sha256": "6618c8a8dc6944b135502eb827301b84a9dcd84c9f075e9680b00f419fb6c234", "mtime_iso": "2025-10-01T08:11:33.818260+00:00", "preview": "import json, os, re, time, pathlib, hashlib\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Dict, Optional, Tuple\nimport yaml\n\n# Mutation anchor: integrate if present, else no-op\ntry:\n    from optimizer.mutationanchor import MutationAnchor as _Anchor\nexcept Exception:\n    class _Anchor:\n        def __init__(self): pass\n        def record(self, kind: str, payload: Dict, parent_id: Optional[str]=None):\n            return type(\"Ev\", (), {\"event_id\": f\"noop-{int(time.time())}\"})\n\nROOT = pathl"}
{"path": "optimizer/cli/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.518641+00:00", "preview": ""}
{"path": "optimizer/cli/main.py", "size_bytes": 1217, "lines": 48, "lang": "python", "sha256": "c3ce7d2301ae12dafe454aef3be82f4fbffec5f2831fb3237c78eae053719d41", "mtime_iso": "2025-10-01T08:02:51.518641+00:00", "preview": "import click\nimport os\n\nfrom optimizer.config.settings import load_config\nfrom optimizer.core.engine import Engine\nfrom optimizer.logging_config import setup_logging, get_logger\n\n\n@click.group()\ndef cli():\n    \"\"\"Optimizer CLI for running simulations.\"\"\"\n    pass\n\n\n@cli.command()\n@click.option(\n    \"--config-path\", default=\"config.yml\", help=\"Path to the configuration file.\"\n)\ndef run(config_path):\n    \"\"\"\n    Run a simulation.\n    \"\"\"\n    if not os.path.exists(config_path):\n        click.echo(f\"Error: Conf"}
{"path": "optimizer/config/settings.py", "size_bytes": 1175, "lines": 48, "lang": "python", "sha256": "26844f6947e50bd250747beb34f2d604f514f24d4a64bbec1256f7c2542f3cfc", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "import yaml\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass SimulationSettings(BaseModel):\n    engine: str = \"pybullet\"\n    gravity: float = -9.8\n    time_step: float = 0.01\n\n\nclass APISettings(BaseModel):\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n\n\nclass LoggingSettings(BaseModel):\n    level: str = \"INFO\"\n    format: str = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    file: Optional[str] = \"optimizer.log\"\n\n\nclass Settings(BaseModel):\n    simulation: SimulationSet"}
{"path": "optimizer/core/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": ""}
{"path": "optimizer/core/auth_matrix.py", "size_bytes": 1554, "lines": 49, "lang": "python", "sha256": "92cfec62148b8b501d47fb31d6089784f1205a96110f1ae0ec3616a060d879e4", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "import networkx as nx\n\nfrom optimizer.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass AuthMatrix:\n    \"\"\"\n    Manages node-to-node credential checks using a graph.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the authentication matrix.\n        \"\"\"\n        self.graph = nx.DiGraph()\n        logger.info(\"Authentication matrix initialized.\")\n\n    def add_credential(self, source_node_id: str, target_node_id: str):\n        \"\"\"\n        Adds a credential from a source nod"}
{"path": "optimizer/core/engine.py", "size_bytes": 1528, "lines": 52, "lang": "python", "sha256": "a5a46f94adffce8d709ba7e38eb70400c9f037344be4d564437846480a682168", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "import pybullet as p\nimport time\n\nfrom optimizer.config.settings import settings\nfrom optimizer.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass Engine:\n    \"\"\"\n    An adapter for the PyBullet physics engine.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the physics engine.\n        \"\"\"\n        self.physics_client = p.connect(p.DIRECT)\n        p.setGravity(0, 0, settings.simulation.gravity)\n        p.setTimeStep(settings.simulation.time_step)\n        logger.info(\"PyB"}
{"path": "optimizer/core/node.py", "size_bytes": 948, "lines": 33, "lang": "python", "sha256": "4e54089a56d587de252e24dbe46670831e374a5e92b37766e7dcce7d12e1c95c", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "from typing import Dict, Any\n\n\nclass Node:\n    \"\"\"\n    Represents a virtual simulation node.\n    \"\"\"\n\n    def __init__(self, node_id: str, position: tuple, metadata: Dict[str, Any] = None):\n        \"\"\"\n        Initializes a Node.\n\n        Args:\n            node_id (str): The unique identifier for the node.\n            position (tuple): The (x, y, z) coordinates of the node.\n            metadata (Dict[str, Any], optional): Additional data associated with the node. Defaults to None.\n        \"\"\"\n        self.n"}
{"path": "optimizer/flaws/broken_imports.py", "size_bytes": 173, "lines": 5, "lang": "python", "sha256": "ff7f09ec93d5bfbd98c378f10d8bd15a84b40a5287b11321ce7cf29f025c1522", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "# FLAWMODE: Broken Imports\n# This module is designed to fail upon import.\n# It simulates a common dependency or packaging error.\n\nimport non_existent_module_for_jules_to_fix"}
{"path": "optimizer/flaws/buginjectionmatrix.py", "size_bytes": 2005, "lines": 47, "lang": "python", "sha256": "231a824d442893259fc5d39e26d9b9c1dac8e74f68a24dd65b46b35b3c86c83a", "mtime_iso": "2025-10-01T08:02:51.522641+00:00", "preview": "# FLAWMODE: Bug Injection Matrix\n# This module defines the programmable flaws that Jules can inject into the system.\n# Each function represents a specific type of failure, designed to be traced and healed.\n\nimport random\n\nclass BugInjectionMatrix:\n    \"\"\"\n    A matrix of controlled bug injections.\n    Each method introduces a specific, traceable flaw.\n    \"\"\"\n\n    def __init__(self, seed=None):\n        self.random = random.Random(seed)\n\n    def cause_null_pointer_exception(self, target_object):\n        \"\"\"I"}
{"path": "optimizer/flaws/infiniteloopsimulator.py", "size_bytes": 1175, "lines": 30, "lang": "python", "sha256": "e589a45601a338a14c43064d049fae4b39ab0cbb7218f8130b1440a7019b1aa1", "mtime_iso": "2025-10-01T08:02:51.526641+00:00", "preview": "# FLAWMODE: Infinite Loop Simulator\n# This module contains code that, if executed, would result in an infinite loop.\n# It is designed to be statically analyzed by Jules to identify and flag\n# non-terminating logic without actually executing it.\n\ndef simulate_infinite_loop():\n    \"\"\"\n    A function containing a deliberate, but non-executed, infinite loop.\n    Jules should be able to identify this pattern without running the code.\n    \"\"\"\n    print(\"SIMULATION: An infinite loop would start here.\")\n    # The f"}
{"path": "optimizer/flaws/memory_poisoner.py", "size_bytes": 1687, "lines": 38, "lang": "python", "sha256": "e2a8e6e2dc6261c8562cf40aeb4554df9cd58f9290682321a64f90f1fb7e7ec8", "mtime_iso": "2025-10-01T08:02:51.526641+00:00", "preview": "# FLAWMODE: Memory Poisoner\n# This module simulates flaws that corrupt the agent's internal state or memory.\n# It tests the agent's ability to detect and recover from corrupted memory,\n# which could be caused by parsing errors, data degradation, or other anomalies.\n\ndef corrupt_memory_json(memory_dict):\n    \"\"\"\n    Simulates corruption of a JSON-like memory structure (a Python dict).\n    It might replace a valid value with one of a different, unexpected type.\n    \"\"\"\n    if \"agent_state\" in memory_dict:\n   "}
{"path": "optimizer/flaws/semanticdriftgenerator.py", "size_bytes": 1551, "lines": 39, "lang": "python", "sha256": "34341967d28a5ee281a66e65731289597513804881e6bcfe24b3659ee657f028", "mtime_iso": "2025-10-01T08:02:51.526641+00:00", "preview": "# FLAWMODE: Semantic Drift Generator\n# This module is designed to introduce subtle logical flaws by altering\n# the meaning of data or code constructs. It tests the agent's ability\n# to perform deeper semantic analysis.\n\ndef invert_boolean_logic(condition):\n    \"\"\"\n    Simulates a flaw where a boolean condition is inverted.\n    For example, `if (x > 5)` becomes `if not (x > 5)`.\n    \"\"\"\n    print(f\"SEMANTIC DRIFT: Inverting boolean logic for condition '{condition}'\")\n    return not condition\n\n\ndef off_by_one"}
{"path": "optimizer/logging_config.py", "size_bytes": 595, "lines": 25, "lang": "python", "sha256": "8cdacdc7ece9aa8f6dc533c6defcfcdeb265d5f4775b2af0e28bc86cd065e50e", "mtime_iso": "2025-10-01T08:02:51.526641+00:00", "preview": "import logging\nimport sys\nfrom optimizer.config.settings import settings\n\n\ndef setup_logging():\n    \"\"\"\n    Set up logging for the application.\n    \"\"\"\n    log_level = getattr(logging, settings.logging.level.upper(), logging.INFO)\n\n    handlers = [logging.StreamHandler(sys.stdout)]\n    if settings.logging.file:\n        handlers.append(logging.FileHandler(settings.logging.file))\n\n    logging.basicConfig(\n        level=log_level, format=settings.logging.format, handlers=handlers\n    )\n\n\ndef get_logger(name: s"}
{"path": "optimizer/research/manifest.yaml", "size_bytes": 269, "lines": 6, "lang": "yaml", "sha256": "e0917cfe30cd81243a69ea0e012c00b3e5fd3de18ba309dcfd6f1d1e3e21eb2a", "mtime_iso": "2025-10-01T08:11:33.818260+00:00", "preview": "# Optional path-remap examples. Fill in as you consolidate PRs.\nmappings:\n  # - from: \"jules/core/agent.py\"\n  #   to:   \"optimizer/agentkits/jules_v1/agent.py\"\n  # - from: \"services/synapse_cortex/main.py\"\n  #   to:   \"optimizer/orchestrators/synapse_cortex/service.py\""}
{"path": "optimizer/research/tree_config.yaml", "size_bytes": 559, "lines": 23, "lang": "yaml", "sha256": "a8bc7905d961c4eb99482534d467561bb2a079fc71d0059341072457edef25d5", "mtime_iso": "2025-10-01T08:11:33.818260+00:00", "preview": "# What to include/exclude and how to hash.\nexclude_globs:\n  - \".git/**\"\n  - \".venv/**\"\n  - \"venv/**\"\n  - \"__pycache__/**\"\n  - \"node_modules/**\"\n  - \"audit/**\"\n  - \"docs/citations/**\"\n  - \".pytest_cache/**\"\n  - \".mypy_cache/**\"\n  - \".ruff_cache/**\"\n\n# If true, hash all files up to hash_max_mb.\nhash_large_files: true\nhash_max_mb: 50\n\n# If true, store short text preview for small text files.\nstore_preview: true\npreview_max_bytes: 512\n\n# Optional: path remap manifest (populate if you moved files between PRs)\nma"}
{"path": "optimizer/research/tree_mapper.py", "size_bytes": 9286, "lines": 260, "lang": "python", "sha256": "2f5ccfba0df0723f0d5f893e198afb741a6e13b6abfe0e4396578becd9ca98dc", "mtime_iso": "2025-10-01T08:11:33.818260+00:00", "preview": "import csv, hashlib, json, os, pathlib, sys, time, io\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Tuple\nimport yaml\nfrom datetime import datetime, timezone\n\n# Optional Mission Ω lineage (no-op if absent)\ntry:\n    from optimizer.mutationanchor import MutationAnchor  # type: ignore\nexcept Exception:  # pragma: no cover\n    class MutationAnchor:  # minimal stub\n        def record(self, kind: str, payload: dict, parent_id: Optional[str]=None):\n            return type(\"E\","}
{"path": "optimizer/utils/__init__.py", "size_bytes": 15, "lines": 1, "lang": "python", "sha256": "f215a7ecf74ec78aa5eac299e2cfb7a34ad13016836e1cfd6638b6e14f444c75", "mtime_iso": "2025-10-01T08:02:51.526641+00:00", "preview": "# Utils module\n"}
{"path": "optimizer/utils/build_helper.py", "size_bytes": 8022, "lines": 265, "lang": "python", "sha256": "a4a511f3336c147d6f5feeba9eb7f5708e17f528c419cac3cb2d58a24d47e380", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": "\"\"\"\nBuild helper utility to detect missing system dependencies.\n\nThis module provides functionality to detect when builds fail due to missing\nsystem dependencies and suggests installation commands for required packages.\n\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List, Tuple, Optional\n\n\nclass BuildDependencyError(Exception):\n    \"\"\"Raised when required build dependencies are missing.\"\"\"\n\n    pass\n\n\nclass DependencyChecker:\n    \"\"\"Check for missing system dependencies and suggest installation comman"}
{"path": "optimizer_sentinel/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": ""}
{"path": "optimizer_sentinel/adapters/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": ""}
{"path": "optimizer_sentinel/adapters/anomalymodeladapter.py", "size_bytes": 293, "lines": 9, "lang": "python", "sha256": "0fab7c761488e0977f734929d1bae61e7cccb025a35c706b45e3315650caec99", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": "class AnomalyModel:\n    def predict(self, trace: str) -> dict:\n        # Simulated ML output\n        return {\n            \"bugtype\": \"racecondition\",\n            \"severity\": 0.92,\n            \"suggested_fix\": \"Add SERIALIZABLE isolation level\",\n            \"target\": \"db/session.py\"\n        }\n"}
{"path": "optimizer_sentinel/adapters/dbtraceadapter.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": ""}
{"path": "optimizer_sentinel/adapters/risk_model.py", "size_bytes": 391, "lines": 11, "lang": "python", "sha256": "6a1f28402b9078d0af900e872ee9a48e98e7ffac7746290536259f204d38694a", "mtime_iso": "2025-10-01T08:02:51.530641+00:00", "preview": "# This is a scaffold for the RiskModel.\n# In a real implementation, this would use a machine learning model\n# to predict the risk based on the context.\nclass RiskModel:\n    def score(self, context):\n        \"\"\"\n        Scores the risk of a given context.\n        Placeholder for now. Returns a fixed score.\n        \"\"\"\n        print(f\"Scoring risk for context: {context}\")\n        return 0.9"}
{"path": "optimizer_sentinel/agents/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.534641+00:00", "preview": ""}
{"path": "optimizer_sentinel/agents/patch_engine.py", "size_bytes": 422, "lines": 11, "lang": "python", "sha256": "cedf327400cddebef29344932fa24a6806442cef49c711b0b4a9755907d5d76e", "mtime_iso": "2025-10-01T08:02:51.534641+00:00", "preview": "# This is a scaffold for the PatchEngine.\n# In a real implementation, this would use a large language model\n# or other generative AI to create a patch.\nclass PatchEngine:\n    def suggest(self, context):\n        \"\"\"\n        Suggests a patch based on the given context.\n        Placeholder for now. Returns a fixed patch.\n        \"\"\"\n        print(f\"Suggesting patch for context: {context}\")\n        return \"suggested_patch\""}
{"path": "optimizer_sentinel/agents/predictive_debug_engine.py", "size_bytes": 1789, "lines": 45, "lang": "python", "sha256": "c4637f74c67eb5fc9998da568af37ccebc10e6e66cb1fb77901c413671c99fea", "mtime_iso": "2025-10-01T08:02:51.534641+00:00", "preview": "class PredictiveDebugEngine:\n    def __init__(self, risk_model, patch_engine, neo4j_anchor):\n        self.risk_model = risk_model\n        self.patch_engine = patch_engine\n        self.neo4j_anchor = neo4j_anchor\n\n    def predict_failure(self, directive):\n        \"\"\"\n        Predicts potential failures based on a directive, using a risk model.\n        If the risk is high, it suggests a patch and logs the prediction.\n        \"\"\"\n        context = self._load_context(directive)\n        risk_score = self.risk_mo"}
{"path": "optimizer_sentinel/agents/predictive_debugger_legacy.py", "size_bytes": 465, "lines": 13, "lang": "python", "sha256": "6a2dfa9f9db654b6e01a5130e585738a2be7502020c801bb0f953f127d7cc525", "mtime_iso": "2025-10-01T08:02:51.534641+00:00", "preview": "from ..adapters.anomalymodeladapter import AnomalyModel\nfrom ..workflows.forecastandpatch import forecastandpatch\nfrom ..storage.neo4j_anchor import anchor_forecast\n\nclass PredictiveDebugger:\n    def __init__(self):\n        self.model = AnomalyModel()\n\n    def monitor_trace(self, trace: str, source: str):\n        forecast = self.model.predict(trace)\n        patch = forecastandpatch(forecast)\n        anchor_forecast(source, forecast, patch)\n        return patch\n"}
{"path": "optimizer_sentinel/interfaces/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.534641+00:00", "preview": ""}
{"path": "optimizer_sentinel/interfaces/cli.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": ""}
{"path": "optimizer_sentinel/main.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": ""}
{"path": "optimizer_sentinel/storage/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": ""}
{"path": "optimizer_sentinel/storage/forecast_cache.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": ""}
{"path": "optimizer_sentinel/storage/neo4j_anchor.py", "size_bytes": 1185, "lines": 41, "lang": "python", "sha256": "6afa098be93eaa8f94f4df3136329f243d1d81c93743728f4b8f7edc03758651", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": "class Neo4jAnchor:\n    def __init__(self, driver):\n        self.driver = driver\n\n    def log_prediction(self, directive, patch):\n        \"\"\"\n        Logs a prediction to Neo4j.\n        \"\"\"\n        query = \"\"\"\n        MERGE (p:Prediction {directive: $directive})\n        SET p.patch = $patch, p.timestamp = timestamp()\n        \"\"\"\n        with self.driver.session() as session:\n            session.run(query, directive=directive, patch=patch)\n        print(f\"Logged prediction for directive: {directive}\")\n\n    de"}
{"path": "optimizer_sentinel/workflows/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": ""}
{"path": "optimizer_sentinel/workflows/forecastandpatch.py", "size_bytes": 622, "lines": 18, "lang": "python", "sha256": "4a9ca47591c3cfb59adf272e1ea6f8a2ec40ecd005597b1839897b4068c5e340", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": "from .schemamigrationguard import SchemaMigrationGuard\n\nschemamigrationguard = SchemaMigrationGuard()\n\ndef inject_isolation(target):\n    print(f\"Injecting SERIALIZABLE isolation level into {target}\")\n\ndef addresourcelimits():\n    print(\"Adding resource limits\")\n\ndef forecastandpatch(forecast: dict) -> str:\n    if forecast[\"bugtype\"] == \"racecondition\":\n        inject_isolation(forecast[\"target\"])\n    elif forecast[\"bugtype\"] == \"oom\":\n        addresourcelimits()\n    elif forecast[\"bugtype\"] == \"migration\":\n"}
{"path": "optimizer_sentinel/workflows/schemamigrationguard.py", "size_bytes": 851, "lines": 25, "lang": "python", "sha256": "3a3e09355822f8f9c93824374e33b45bf212a5d19e801c0dee1079ff70890c89", "mtime_iso": "2025-10-01T08:02:51.538641+00:00", "preview": "from typing import Dict, Any\n\nclass SchemaMigrationGuard:\n    \"\"\"\n    A guard to validate schema migrations and detect potential conflicts.\n    \"\"\"\n\n    def validate(self, target: Dict[str, Any]) -> bool:\n        \"\"\"\n        Validates the schema migration target.\n\n        In a real implementation, this would involve checking for:\n        - Concurrent migrations on the same table.\n        - Migrations that lock critical tables for extended periods.\n        - Backwards-incompatible changes.\n\n        For this "}
{"path": "physixiology_api/gateway/Dockerfile", "size_bytes": 638, "lines": 23, "lang": "text", "sha256": "49add3494c2338de76ce3a72f1d08ba3784588ae43cacbd1dbcb3da64281d554", "mtime_iso": "2025-10-01T08:02:51.542641+00:00", "preview": "# Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the dependencies file to the working directory\nCOPY requirements.txt .\n\n# Install any needed dependencies specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the content of the local src directory to the working directory\nCOPY main.py .\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment varia"}
{"path": "physixiology_api/gateway/main.py", "size_bytes": 332, "lines": 14, "lang": "python", "sha256": "a84b481ed61eb5c387ea038e755c821cd6764423cc8cd0311f8a7d9b3ec12503", "mtime_iso": "2025-10-01T08:02:51.542641+00:00", "preview": "from fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Physixiology API Gateway\",\n    description=\"Interface for agentic systems and verification.\",\n    version=\"0.1.0\",\n)\n\n@app.get(\"/health\", status_code=200)\nasync def health_check():\n    \"\"\"\n    Endpoint to verify that the API is running.\n    \"\"\"\n    return {\"status\": \"healthy\"}"}
{"path": "physixiology_api/gateway/requirements.txt", "size_bytes": 25, "lines": 2, "lang": "text", "sha256": "0a4bcad277f760e870965be881ea3bed85f88eddf0f5b598ca8c8076a71ab978", "mtime_iso": "2025-10-01T08:02:51.542641+00:00", "preview": "fastapi\nuvicorn[standard]"}
{"path": "policies/merge_control.rego", "size_bytes": 1251, "lines": 47, "lang": "text", "sha256": "b7dbe4ba1143473daf6dcd79d1613f78a5d4d031b4d0e5a0dba20cf6c19447d1", "mtime_iso": "2025-10-01T08:02:51.542641+00:00", "preview": "package synapse.cortex.merge_control\n\ndefault allow_merge = false\n\n# Input structure expected:\n# {\n#   \"security_scan\": { \"status\": \"CLEAN\", \"high_severity\": 0 },\n#   \"audit_anchor\": { \"is_anchored\": true, \"ledger_tx_id\": \"...\" },\n#   \"requester_role\": \"engineer\" | \"security_officer\"\n# }\n\n# Rule 1: Allow if all mandatory checks pass\nallow_merge {\n    is_security_clean\n    is_audit_anchored\n}\n\n# Rule 2: Allow if security failed but overridden by authorized role\nallow_merge {\n    not is_security_clean\n    is_"}
{"path": "psi_agent/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": ""}
{"path": "psi_agent/agents/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": ""}
{"path": "psi_agent/agents/db_evolver.py", "size_bytes": 706, "lines": 22, "lang": "python", "sha256": "04de998ed61a598201a7c2f693ff6de0ae5124b2f498697435488c13cb8946e4", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": "import logging\n\nclass DbEvolver:\n    \"\"\"\n    Scaffold for the Database Evolver agent.\n    This agent is responsible for managing database schemas and data migrations.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        logging.info(\"DbEvolver initialized.\")\n\n    def run(self):\n        \"\"\"\n        Execute the database evolution tasks.\n        \"\"\"\n        logging.info(\"Running database evolution tasks (placeholder)...\")\n        # In the future, this will contain logic to:\n        # - "}
{"path": "psi_agent/agents/fallback_repair.py", "size_bytes": 816, "lines": 24, "lang": "python", "sha256": "4437be4fdd4b3d91b6d990011ecc8742edd0a08ef6295c045de7044b9d7a75dd", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": "import logging\n\nclass FallbackRepair:\n    \"\"\"\n    Scaffold for the Fallback Repair agent.\n    This agent is triggered when the SelfDebugger detects a critical\n    issue. It is responsible for taking corrective actions to restore\n    the agent to a functional state.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        logging.info(\"FallbackRepair initialized.\")\n\n    def run(self):\n        \"\"\"\n        Execute the fallback repair procedures.\n        \"\"\"\n        logging.info(\"Running fal"}
{"path": "psi_agent/agents/knowledge_expander.py", "size_bytes": 750, "lines": 22, "lang": "python", "sha256": "e77db7a227b5f45c0285dd841eacd2e98080032c877c3592fd0ff0bdabbd0a14", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": "import logging\n\nclass KnowledgeExpander:\n    \"\"\"\n    Scaffold for the Knowledge Expander agent.\n    This agent is responsible for gathering information, such as\n    crawling API documentation or scanning new data sources.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        logging.info(\"KnowledgeExpander initialized.\")\n\n    def run(self):\n        \"\"\"\n        Execute the knowledge expansion tasks.\n        \"\"\"\n        logging.info(\"Running knowledge expansion tasks (placeholder)...\")\n"}
{"path": "psi_agent/agents/repo_rewriter.py", "size_bytes": 752, "lines": 23, "lang": "python", "sha256": "e6551880bbc24aec031897006aebe2c664c168079c8d79355dc6e4c26fcb48d2", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": "import logging\n\nclass RepoRewriter:\n    \"\"\"\n    Scaffold for the Repo Rewriter agent.\n    This agent is responsible for making changes to the codebase,\n    such as applying patches, fixing bugs, or refactoring code.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        logging.info(\"RepoRewriter initialized.\")\n\n    def run(self):\n        \"\"\"\n        Execute the repository rewriting tasks.\n        \"\"\"\n        logging.info(\"Running repository rewriting tasks (placeholder)...\")\n        #"}
{"path": "psi_agent/agents/self_debugger.py", "size_bytes": 770, "lines": 24, "lang": "python", "sha256": "b92a7a6c854b15a5e206db3421a35bc592d602c1455f0334967e889ca9267efa", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": "import logging\n\nclass SelfDebugger:\n    \"\"\"\n    Scaffold for the Self-Debugger agent.\n    This agent is responsible for detecting issues within the agent\n    itself, such as stale locks, repeated errors, or performance\n    degradation.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        logging.info(\"SelfDebugger initialized.\")\n\n    def run(self):\n        \"\"\"\n        Execute the self-debugging checks.\n        \"\"\"\n        logging.info(\"Running self-debugging checks (placeholder)...\")"}
{"path": "psi_agent/alignment_monitor/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": ""}
{"path": "psi_agent/alignment_monitor/benchmarking/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.546641+00:00", "preview": ""}
{"path": "psi_agent/alignment_monitor/benchmarking/benchmark_exposer.py", "size_bytes": 2718, "lines": 66, "lang": "python", "sha256": "0fde21ca456c28ba60982eedd7f408e076cfade51e280f756dbddb42cefdef5a", "mtime_iso": "2025-10-01T08:02:51.550641+00:00", "preview": "\"\"\"\nThe core of the Ruthless Benchmarking system.\n\nThis module is responsible for providing total, unflinching transparency about\nthe performance of the psi_agent system. It generates a periodic report that\nexposes both internal performance metrics (e.g., latency, error rates) and\nexternal intelligence about the flaws and contamination in public benchmarks.\nThis report is the foundational data source for the FlawDetector and the\nAutoAligner, enabling the system to reason about its own performance and risks."}
{"path": "psi_agent/alignment_monitor/benchmarking/flaw_detector.py", "size_bytes": 2403, "lines": 59, "lang": "python", "sha256": "de64c835ffc70e3e83d59c27047ce0d5dba882c45f15ea9a894ace0452027ce1", "mtime_iso": "2025-10-01T08:02:51.550641+00:00", "preview": "\"\"\"\nThe analytical component of the benchmarking system.\n\nThis module is responsible for analyzing the raw data from the BenchmarkExposer\nto identify specific, actionable risks that could lead to system degradation or\nentropy collapse. It applies a set of predefined heuristics and thresholds to\nthe benchmark report to detect patterns like high recursion depth, excessive\nlatency, or reliance on externally flagged agents. Its output is the direct\ntrigger for the AutoAligner's mutation proposals.\n\"\"\"\nimport lo"}
{"path": "psi_agent/alignment_monitor/integration/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.550641+00:00", "preview": ""}
{"path": "psi_agent/alignment_monitor/integration/auto_aligner.py", "size_bytes": 2305, "lines": 54, "lang": "python", "sha256": "28e0730940ee1bfbf6e5a6d5cc95c1c78457e25c32348b500444a60ce2f96e7e", "mtime_iso": "2025-10-01T08:02:51.550641+00:00", "preview": "\"\"\"\nThe core of the closed-loop, self-healing system.\n\nThis module connects the system's analytical capabilities (the FlawDetector)\nwith its capacity for self-modification (the MutationEngine). It is the final\ndecision-maker in the loop, responsible for analyzing the detected risks and\norchestrating the response. It selects the appropriate type of mutation,\ninvokes the mutation engine to generate the patch, and then uses the PR\nsimulator to safely apply the change to the codebase.\n\"\"\"\nimport logging\n\n# Conc"}
{"path": "psi_agent/alignment_monitor/integration/timeline_tracker.py", "size_bytes": 1414, "lines": 33, "lang": "python", "sha256": "19821316943ffb62231ddc3f00f68e40021319cba30460c39705fcdd242905a2", "mtime_iso": "2025-10-01T08:02:51.550641+00:00", "preview": "\"\"\"\nA module for monitoring shifts in community consensus on AI timelines.\n\nThis component provides a longitudinal view of the intelligence gathered by the\nsystem. It is responsible for querying the Neo4j graph database to track how\npredictions and sentiment about AI alignment evolve over time. While the\nAutoAligner responds to immediate, acute risks, the TimelineTracker is designed\nto detect slower-moving, strategic shifts in the landscape, providing crucial\ncontext for long-term governance of the psi_agen"}
{"path": "psi_agent/alignment_monitor/processing/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": ""}
{"path": "psi_agent/alignment_monitor/processing/prediction_parser.py", "size_bytes": 1933, "lines": 52, "lang": "python", "sha256": "676a7f909e9b1a198dccf12f506fcfd587775c1e4a95e52cfd9f9bf05658b71f", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nA parser for extracting structured data from LLM-generated text.\n\nThis module is a critical component of the synthesis layer. It takes the\nsemi-structured text output produced by an LLM agent and parses it into a\nclean, predictable dictionary format. It uses regular expressions to robustly\nextract key-value pairs and JSON-formatted data for entities like predictions\nand benchmarks, making the intelligence usable by downstream systems like the\nNeo4j anchor and the flaw detector.\n\"\"\"\nimport logging\nimport"}
{"path": "psi_agent/alignment_monitor/processing/synthesizer.py", "size_bytes": 1977, "lines": 50, "lang": "python", "sha256": "4a857a35a5b32c2b78194f73c5ba690b8c5bf1d514c7343e0e9dead83eace88a", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nThe core of the intelligence processing layer.\n\nThis module is responsible for taking the raw, unstructured data collected by\nthe scanner and transforming it into structured, actionable intelligence. It\nleverages agentic LLMs (e.g., Claude, GPT) via a routing mechanism to analyze\nthe content, summarize key arguments, and extract specific, structured information\nlike predictions, benchmarks, and citations.\n\"\"\"\nimport logging\n\n# Conceptual import of the parser and agent router\nfrom .prediction_parser impo"}
{"path": "psi_agent/alignment_monitor/scanner/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": ""}
{"path": "psi_agent/alignment_monitor/scanner/alignment_scanner.py", "size_bytes": 2816, "lines": 70, "lang": "python", "sha256": "f0d3201beadace22f5faf89160dc2241ef3f43c8c027e5cabe4fae179dc764ac", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nThe central orchestrator for the Daily Scanner Module.\n\nThis module is responsible for initiating and managing the daily scan across all\nconfigured alignment-related data sources. It uses concurrent execution to run\nall scrapers efficiently. A key feature is its mutation awareness: it fingerprints\neach scraper module before execution to ensure that any changes to the scrapers\nthemselves are tracked in the system's lineage, supporting full auditability.\n\"\"\"\nimport asyncio\nimport time\nimport logging\n\n# Co"}
{"path": "psi_agent/alignment_monitor/scanner/source_aifutures.py", "size_bytes": 754, "lines": 19, "lang": "python", "sha256": "a88ddb288c3717373da28111d708114e09b4dfb0041fc21cd849b836c13c6a7f", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nPlaceholder for the AI Futures and timeline prediction website scraper.\n\nThis module would be responsible for scraping websites that aggregate\npredictions about the future of AI, such as Metaculus or other community\nforecasting platforms. A real implementation would need to parse structured\ndata like prediction graphs, timelines, and probability distributions.\n\"\"\"\nimport logging\n\nclass AiFuturesScraper:\n    \"\"\"A conceptual scraper for AI prediction websites.\"\"\"\n    def __init__(self):\n        self.name "}
{"path": "psi_agent/alignment_monitor/scanner/source_arxiv.py", "size_bytes": 738, "lines": 20, "lang": "python", "sha256": "8e4d1e97edd53916811dec34a1a680755d132b26fdcc127315f5c96ec18bbcf8", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nPlaceholder for the ArXiv scraper.\n\nA real implementation of this module would use the official ArXiv API to\nquery for new publications in relevant categories (e.g., cs.AI, cs.LG, stat.ML).\nIt would be responsible for fetching paper metadata, including abstracts,\nauthors, and publication dates, which serve as crucial inputs for the\nsynthesis layer.\n\"\"\"\nimport logging\n\nclass ArxivScraper:\n    \"\"\"A conceptual scraper for the ArXiv pre-print repository.\"\"\"\n    def __init__(self):\n        self.name = \"ArXiv"}
{"path": "psi_agent/alignment_monitor/scanner/source_hn_reddit.py", "size_bytes": 749, "lines": 19, "lang": "python", "sha256": "1e558c8b7e49230c461c763be0f62e4045f0648da552d175f5a2f2cc001ec3c1", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nPlaceholder for the Hacker News and Reddit scraper.\n\nA real implementation would likely use the official APIs provided by Reddit\nand Hacker News (via Firebase) to ensure reliable and sanctioned access to\nthe data. This module would be responsible for querying relevant communities\n(e.g., r/singularity, r/agi) and threads for discussions related to AI alignment.\n\"\"\"\nimport logging\n\nclass HnRedditScraper:\n    \"\"\"A conceptual scraper for Hacker News and Reddit.\"\"\"\n    def __init__(self):\n        self.name ="}
{"path": "psi_agent/alignment_monitor/scanner/source_lw_af.py", "size_bytes": 726, "lines": 19, "lang": "python", "sha256": "406c2df681a2d0c52d4a70cbad6f45cc754853a9fb910116f4dc6cc9969a47c6", "mtime_iso": "2025-10-01T08:02:51.554641+00:00", "preview": "\"\"\"\nPlaceholder for the LessWrong and Alignment Forum scraper.\n\nIn a real implementation, this module would contain the logic to connect to\nthese websites, handle logins if necessary, and parse the HTML content to\nextract relevant posts, comments, and metadata. It would be designed to be\nresilient to minor changes in website layout.\n\"\"\"\nimport logging\n\nclass LwAfScraper:\n    \"\"\"A conceptual scraper for LessWrong and the Alignment Forum.\"\"\"\n    def __init__(self):\n        self.name = \"LessWrong/AlignmentForu"}
{"path": "psi_agent/daemon/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.558641+00:00", "preview": ""}
{"path": "psi_agent/daemon/vm_daemon.py", "size_bytes": 4096, "lines": 135, "lang": "python", "sha256": "8d11248c7b9cc1a88a3bcdd5d8b6a8c320665e95b9b270caa0822eb22139f5f9", "mtime_iso": "2025-10-01T08:02:51.558641+00:00", "preview": "import argparse\nimport logging\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\n# Add project root to sys.path to allow imports from other modules\nproject_root = Path(__file__).resolve().parents[2]\nsys.path.insert(0, str(project_root))\n\nimport yaml\n\n# Import utilities\nfrom psi_agent.utils.logging import setup_logging\n\n# Placeholder imports for agent capabilities\n# from psi_agent.agents.knowledge_expander import KnowledgeExpander\n# from psi_agent.agents.repo_rewriter import RepoRewriter\n# from psi_"}
{"path": "psi_agent/memory/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.558641+00:00", "preview": ""}
{"path": "psi_agent/memory/cockroachdb_ledger.py", "size_bytes": 767, "lines": 24, "lang": "python", "sha256": "979dd9b48090aef6ac3765fe9455195e5a0824dff40eb841fb2bda9993f539c3", "mtime_iso": "2025-10-01T08:02:51.558641+00:00", "preview": "\"\"\"\nPlaceholder for the CockroachDB Ledger.\n\nThis module is responsible for all interactions with the CockroachDB instance.\nIt handles the storage of time-series data, such as scan history, benchmark\nreports, and other event-based logs that benefit from a distributed SQL\ndatabase's resilience and consistency.\n\"\"\"\n\nclass CockroachDBLedger:\n    \"\"\"\n    A conceptual class for anchoring time-series data into CockroachDB.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializes the connection to the database.\"\"\"\n "}
{"path": "psi_agent/memory/neo4j_anchor.py", "size_bytes": 854, "lines": 25, "lang": "python", "sha256": "fbe0b2ed8b013d912fc805544e14f66db0676694e1480e4d0c228113bc5fdf8e", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": "\"\"\"\nPlaceholder for the Neo4j Anchor.\n\nThis module is responsible for all interactions with the Neo4j graph database.\nIt is designed to store and manage the complex, interconnected relationships\nbetween various entities within the AI alignment ecosystem. This includes\ncitation graphs, prediction lineage, benchmark-flaw relationships, and the\nforensic trail of agent mutations.\n\"\"\"\n\nclass Neo4jAnchor:\n    \"\"\"\n    A conceptual class for anchoring relational data into the Neo4j graph.\n    \"\"\"\n    def __init__(s"}
{"path": "psi_agent/mutation/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": ""}
{"path": "psi_agent/mutation/fingerprint.py", "size_bytes": 923, "lines": 25, "lang": "python", "sha256": "276ee10586380a864bb3ccd3a1348e30957776d789b7a32f30c8493cbf413a07", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": "\"\"\"\nPlaceholder for the Forensic Fingerprinting module.\n\nThis module is responsible for generating deterministic fingerprints (hashes)\nfor various types of data, including raw scraped content, source code modules,\nand agent-generated mutations. These fingerprints are crucial for ensuring\ndata integrity, tracking lineage, and enabling auditable, replayable event\nsequences in the system's memory.\n\"\"\"\n\nclass Fingerprint:\n    \"\"\"\n    A conceptual class for generating deterministic fingerprints.\n    \"\"\"\n    def "}
{"path": "psi_agent/mutation/mutation_engine.py", "size_bytes": 1364, "lines": 30, "lang": "python", "sha256": "d6f1bbfc376d57ee72bef04687a78583d6590d1238a769088528a0176f30cef7", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": "\"\"\"\nA conceptual engine for generating corrective code mutations.\n\nThis module is the \"hands\" of the auto-aligner. It is responsible for\ntranslating the abstract risks identified by the FlawDetector into concrete,\nsyntactically valid code patches. In a real implementation, this might\nleverage a specialized code-generation LLM to create patches that address\nspecific problems like high recursion depth or agent-specific vulnerabilities.\n\"\"\"\nimport logging\n\nclass MutationEngine:\n    \"\"\"Generates code patches to"}
{"path": "psi_agent/orchestration/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": ""}
{"path": "psi_agent/orchestration/pr_simulator.py", "size_bytes": 1217, "lines": 30, "lang": "python", "sha256": "7e563e4cd6bcde2e860fd0144abf114a5ce4faf9fc8f75273a11d3516f99218f", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": "\"\"\"\nA conceptual simulator for the pull request and merge process.\n\nThis module acts as an abstraction layer over the version control system (e.g., Git).\nIt is responsible for taking a proposed code mutation from the MutationEngine,\npackaging it into a pull request, running automated checks (like linting and\nunit tests), and simulating the merge process. This ensures that all self-healing\nchanges are subject to the same quality gates as human-generated code, providing\na crucial layer of safety and auditabil"}
{"path": "psi_agent/orchestration/psi_kernel.py", "size_bytes": 769, "lines": 22, "lang": "python", "sha256": "84ab12bcc4f99736b21387c7c7be124ab55349524593806578580b5e6f578b08", "mtime_iso": "2025-10-01T08:02:51.562641+00:00", "preview": "\"\"\"\nPlaceholder for the Psi-Kernel, the root orchestrator of the system.\n\nThe Psi-Kernel is the central nervous system of the psi_agent architecture.\nIt is responsible for initializing all subsystems, managing the main event\nloop, and orchestrating the flow of information between the scanner,\nsynthesizer, benchmarker, and auto-aligner. It also handles critical,\nlow-level functions like circuit breaking and graceful degradation in response\nto multi-vendor failures.\n\"\"\"\n\nclass PsiKernel:\n    \"\"\"\n    A concept"}
{"path": "psi_agent/utils/__init__.py", "size_bytes": 0, "lines": 0, "lang": "python", "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": ""}
{"path": "psi_agent/utils/logging.py", "size_bytes": 704, "lines": 25, "lang": "python", "sha256": "db3cb18183b120ba72578267127ac485106b2d796d3ca27fc28ab0d4c87d1909", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "import logging\nimport sys\nfrom pathlib import Path\n\ndef setup_logging(log_file_path=\"logs/psi_daemon.log\"):\n    \"\"\"\n    Sets up a standardized logger for the application.\n\n    Args:\n        log_file_path (str): The path to the log file, relative to the project root.\n    \"\"\"\n    project_root = Path(__file__).resolve().parents[2]\n    log_file = project_root / log_file_path\n\n    log_file.parent.mkdir(exist_ok=True)\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s - %(name)s - %"}
{"path": "pytest.ini", "size_bytes": 23, "lines": 2, "lang": "ini", "sha256": "88c344ea0f814ab7b9108bb818651db34ef2b8c81dc5b5bf6bcd64823038a54a", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "[pytest]\npythonpath = ."}
{"path": "requirements.txt", "size_bytes": 127, "lines": 14, "lang": "text", "sha256": "f84a4a2461684814e1f650cce877af25f5bdee7f9e327afb4b1aae94a4f84350", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "fastapi\nuvicorn[standard]\npydantic\npyyaml\nclick\npytest\nflake8\npybullet\nnetworkx\nhttpx\nopenai\npymilvus\npytest-asyncio\ntemporalio"}
{"path": "run_mission.py", "size_bytes": 585, "lines": 16, "lang": "python", "sha256": "766b046c74d172cca1941dcb01eeb5e6a5c05902b175a389c5fd08ca4ae663ba", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "from jules_mission_omega.julesmissionomega import JulesMissionOmega\n\n# The user-provided prompt that triggers the process\nPROMPT = \"Analyze the provided schematics and generate a technical implementation plan.\"\n\nif __name__ == \"__main__\":\n    # Instantiate the mission orchestrator\n    mission = JulesMissionOmega(prompt=PROMPT)\n\n    # Execute the mission\n    final_response = mission.execute()\n\n    # Print the final, deterministically generated response\n    print(\"\\n>>>> FINAL DETERMINISTIC RESPONSE >>>>\")\n  "}
{"path": "scripts/DirectiveReplayHarness.py", "size_bytes": 1387, "lines": 41, "lang": "python", "sha256": "65fef98c6e48d42fcc20035e423c9984c435de0eb47507047950c0b8cc96f21a", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "# Placeholder for the Directive Replay Harness\n# This script will be used to replay directives for testing and debugging.\n\nclass DirectiveReplayHarness:\n    \"\"\"\n    A harness for replaying directives to test the system's response.\n    \"\"\"\n\n    def __init__(self, directive_path):\n        \"\"\"\n        Initializes the harness with the path to the directive file.\n        \"\"\"\n        self.directive_path = directive_path\n        self.directive = None\n\n    def load_directive(self):\n        \"\"\"\n        Loads the dir"}
{"path": "scripts/commitandmerge.py", "size_bytes": 3826, "lines": 90, "lang": "python", "sha256": "6043dc97ecaf0f680b8c706ac8a63db1966493507b02b1a1b5e61e645f9f279b", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "# FLAWMODE: Commit and Merge Automator\n# This script provides the functions for automating the Git operations\n# required for the agent to commit its own self-healing patches and create\n# a pull request for review.\n\nimport os\nimport subprocess\nimport uuid\n\nclass GitAutomator:\n    \"\"\"\n    Handles Git operations to commit a patch and create a pull request.\n    \"\"\"\n\n    def __init__(self, patch_data):\n        self.patch_data = patch_data\n        self.branch_name = f\"jules-flawfix-{uuid.uuid4().hex[:8]}\"\n\n    de"}
{"path": "scripts/deploy-opa.sh", "size_bytes": 779, "lines": 21, "lang": "bash", "sha256": "0bf699555bab89f6f1ee9b40dfbd88a1c5e6d8dd3b8cae463f2fbfc8440edb46", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "#!/bin/bash\n# This script deploys Open Policy Agent (OPA) to the synapse-system namespace using Helm.\n\n# Exit immediately if a command exits with a non-zero status.\nset -e\n\n# Add the OPA Helm repository\necho \"Adding OPA Helm repository...\"\nhelm repo add open-policy-agent https://open-policy-agent.github.io/opa-helm-charts\nhelm repo update\n\n# Install OPA as a centralized policy service.\n# We disable the admission controller as we are using OPA for application-level policy decisions,\n# not as a Kubernetes adm"}
{"path": "scripts/exploitlineagetracer.py", "size_bytes": 2975, "lines": 83, "lang": "python", "sha256": "96792aced1b33c9a4fb52b8ba4f19f8980aa810c3ef1fc3aedd0f57c5937b620", "mtime_iso": "2025-10-01T08:02:51.566641+00:00", "preview": "# FLAWMODE: Exploit Lineage Tracer\n# This script provides the interface for anchoring the full lineage of a\n# flaw-and-recovery cycle into a Neo4j graph database. This creates an\n# immutable, queryable memory of the agent's learning process.\n\nclass LineageTracer:\n    \"\"\"\n    Connects to a Neo4j database and records the lineage of a healing cycle.\n    \"\"\"\n\n    def __init__(self, neo4j_config):\n        self.config = neo4j_config\n        self.driver = None\n        # In a real implementation, you would initiali"}
{"path": "scripts/failure_tracer.py", "size_bytes": 674, "lines": 19, "lang": "python", "sha256": "60aa73cb598671b696480ea4699431082a9e813c6102cf2b70d9d9330bbaaa4b", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "import json\nimport datetime\n\nclass FailureTracer:\n    \"\"\"\n    Traces and records test failures to a structured format.\n    This is a placeholder for a more sophisticated error analysis engine.\n    \"\"\"\n    def trace(self, test_name, error_message):\n        \"\"\"Records a failure event.\"\"\"\n        trace_data = {\n            \"test\": test_name,\n            \"error\": error_message,\n            \"timestamp\": datetime.datetime.now().isoformat(),\n        }\n        # In a real implementation, this would write to a dedic"}
{"path": "scripts/install_dependencies.sh", "size_bytes": 4175, "lines": 135, "lang": "bash", "sha256": "aecac084712163b740f74344e0cbe9be646eebd14ced6e93dcd6b2a6b7492a5a", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "#!/bin/bash\n# Script to detect missing system dependencies and suggest installation commands\n# This script focuses on build dependencies required for Python packages like pybullet\n\nset -e\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\n# Flag to track if any dependencies are missing\nMISSING_DEPS=0\n\n# Detect package manager\ndetect_package_manager() {\n    if command -v apt-get &> /dev/null; then\n        echo \"apt-get\"\n    elif command -v yum &> /dev/null; "}
{"path": "scripts/install_systemd.sh", "size_bytes": 1761, "lines": 67, "lang": "bash", "sha256": "c7e86fa9fc47bf214cddf6b30addb001f73e3542a84c27716c5925b94db939c0", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "#!/bin/bash\n\n# This script installs the psi-daemon as a systemd service.\n# It should be run with sudo privileges.\n\nset -e\n\n# --- Configuration ---\nSERVICE_NAME=\"psi-daemon\"\nSERVICE_FILE=\"/etc/systemd/system/${SERVICE_NAME}.service\"\nPROJECT_DIR=$(cd \"$(dirname \"$0\")/..\" && pwd)\nVENV_PYTHON=\"${PROJECT_DIR}/.venv/bin/python\"\nDAEMON_SCRIPT=\"${PROJECT_DIR}/psi_agent/daemon/vm_daemon.py\"\nCONFIG_FILE=\"${PROJECT_DIR}/configs/config.yaml\"\nUSER=$(whoami)\n\n# --- Check for root privileges ---\nif [ \"$EUID\" -ne 0 ]; then"}
{"path": "scripts/jules_memory_anchor.py", "size_bytes": 958, "lines": 31, "lang": "python", "sha256": "90c21355d11d7820611139a76279f495a05b1563cf52eca01458429a32c1d0bb", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "import json\nimport datetime\nimport os\n\nclass JulesMemoryAnchor:\n    \"\"\"\n    Anchors the results of a recovery or rewrite cycle into a persistent\n    memory format (JSON files in the memory/ directory).\n    \"\"\"\n    def __init__(self, memory_dir=\"memory\"):\n        self.memory_dir = memory_dir\n        os.makedirs(self.memory_dir, exist_ok=True)\n\n    def store(self, directive_id, context, result, benchmark):\n        \"\"\"\n        Stores a memory node as a JSON file.\n        \"\"\"\n        memory_node = {\n           "}
{"path": "scripts/jules_recursive_loop_engine.py", "size_bytes": 3055, "lines": 75, "lang": "python", "sha256": "3aaa472113bd206b0082eeeaadcbba4d2ce17fe5395cecc28322cb63d6787609", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "from scripts.failure_tracer import FailureTracer\nfrom scripts.patch_benchmark_matrix import PatchBenchmarkMatrix\nfrom scripts.jules_memory_anchor import JulesMemoryAnchor\nfrom scripts.jules_self_rewriter import JulesSelfRewriter\n\nclass JulesRecursiveLoopEngine:\n    \"\"\"\n    Orchestrates the recursive, self-evolving loop of the Jules agent.\n    This engine integrates failure tracing, patch generation, benchmarking,\n    and memory anchoring to enable the system to learn from its actions.\n    \"\"\"\n    def __init"}
{"path": "scripts/jules_self_rewriter.py", "size_bytes": 1363, "lines": 32, "lang": "python", "sha256": "33bbbed177b0b3629192099368263968840624348b587e29c3410a9496a6dc39", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "class JulesSelfRewriter:\n    \"\"\"\n    A placeholder for the self-rewriting agent. In a real implementation,\n    this module would use an AI model to generate patches for its own\n    source code to improve its functionality over time.\n    \"\"\"\n    def rewrite(self, module_path, goal):\n        \"\"\"\n        Reads its own source code, generates a patch, and applies it.\n        \"\"\"\n        print(f\"SELF REWRITE: Attempting to rewrite {module_path} with goal: {goal}\")\n        try:\n            with open(module_path, \""}
{"path": "scripts/julesflawmaprunner.py", "size_bytes": 3015, "lines": 84, "lang": "python", "sha256": "e2cf11b97201b11d095323c91c961d4cc036f69dd99133b193ea600a97bbdd99", "mtime_iso": "2025-10-01T08:02:51.570641+00:00", "preview": "# FLAWMODE: Jules Flaw Map Runner\n# This script is the orchestrator for the FLAWMODE protocol.\n# It selects a flaw from the bug injection matrix and executes it\n# to trigger a failure that the Jules agent must then heal.\n\nimport importlib\nimport random\nimport sys\n\n# Add the optimizer directory to the path to allow importing the flaws\nsys.path.append('.')\n\nfrom optimizer.flaws import buginjectionmatrix\n# from optimizer.flaws import broken_imports # This is intentionally broken and should not be imported dire"}
{"path": "scripts/load-opa-policies.sh", "size_bytes": 1187, "lines": 32, "lang": "bash", "sha256": "a4f409e20ea827d9d27f1a6de968c2aeb4ed2d72d52fe0b3326db8dcb486f1b4", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "#!/bin/bash\n# This script loads Rego policies into a Kubernetes ConfigMap for OPA to discover.\n\n# Exit immediately if a command exits with a non-zero status.\nset -e\n\nNAMESPACE=\"synapse-system\"\nPOLICY_DIR=\"policies\"\nCONFIGMAP_NAME=\"opa-policies-merge-control\"\nPOLICY_FILE=\"merge_control.rego\"\n\n# Check if the policy file exists\nif [ ! -f \"${POLICY_DIR}/${POLICY_FILE}\" ]; then\n    echo \"Error: Policy file not found at ${POLICY_DIR}/${POLICY_FILE}\"\n    exit 1\nfi\n\necho \"Creating ConfigMap ${CONFIGMAP_NAME} from $"}
{"path": "scripts/patch_benchmark_matrix.py", "size_bytes": 812, "lines": 22, "lang": "python", "sha256": "7ec5f4732effe7c51a2f1b6362946b3e1b44e8f211dce041b8ec09d546e2b783", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "class PatchBenchmarkMatrix:\n    \"\"\"\n    Scores the quality of a generated patch based on a matrix of metrics.\n    This is a placeholder for a more advanced code quality analysis model.\n    \"\"\"\n    def score(self, patch_code):\n        \"\"\"\n        Scores the patch based on clarity, slop, and token cost.\n        \"\"\"\n        if not patch_code:\n            return {\"clarity\": 0, \"slop_index\": 10, \"token_cost\": 0}\n\n        tokens = patch_code.split()\n        clarity = len(set(tokens)) / len(tokens) if tokens else "}
{"path": "scripts/selfhealingloop.py", "size_bytes": 4154, "lines": 101, "lang": "python", "sha256": "1ed0e900802faae473f9319c9d37952db8faf277cea53e5c36f5df0d1abf4a6c", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "# FLAWMODE: Self-Healing Loop\n# This script defines the core logic for the agent's self-healing process.\n# It orchestrates the full cycle:\n# 1. Trace the failure (receive a flaw trace).\n# 2. Generate a patch to correct the flaw.\n# 3. Benchmark the patch for effectiveness and clarity.\n# 4. Anchor the entire process in memory for future learning.\n\nimport json\n\nclass SelfHealingLoop:\n    \"\"\"\n    Manages the \"trace, patch, benchmark, anchor\" cycle.\n    \"\"\"\n\n    def __init__(self, trace):\n        self.trace = tr"}
{"path": "scripts/verify_gateway.sh", "size_bytes": 1299, "lines": 45, "lang": "bash", "sha256": "9be8a90310b3c6f3188e1a5143e5e51aa1ae5d2c5d0709edc0917b5d84b87d3e", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "#!/bin/bash\n\n# This script verifies the physixiology_api/gateway service.\n# It builds the Docker image, runs the container, checks the health endpoint,\n# and then cleans up.\n\nset -e\n\nGATEWAY_DIR=\"physixiology_api/gateway\"\nIMAGE_NAME=\"physixiology-gateway\"\nCONTAINER_NAME=\"physixiology-gateway-container\"\nHOST_PORT=8000\n\necho \"--- Starting Gateway Verification ---\"\n\n# 1. Build the Docker image\necho \"Building Docker image: $IMAGE_NAME\"\nsudo docker build -t $IMAGE_NAME $GATEWAY_DIR\n\n# 2. Run the Docker container"}
{"path": "sentinel_core/adapters/claude_adapter.py", "size_bytes": 622, "lines": 14, "lang": "python", "sha256": "007f305993a716389cc038239e95accd2b421c4d28dab8228019c9e9cfe87ed8", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "class ClaudeAdapter:\n    def debug_traceback(self, traceback: str) -> str:\n        \"\"\"Placeholder for debugging a traceback with Claude.\"\"\"\n        print(f\"Debugging traceback with Claude: {traceback}\")\n        return f\"Diagnosis for: {traceback}\"\n\n    def diagnose_build_failure(self, traceback: str) -> str:\n        prompt = f\"Diagnose this CI build error and suggest a patch:\\n\\n{traceback}\"\n        return self.invoke(prompt)\n\n    def invoke(self, prompt: str) -> str:\n        \"\"\"Placeholder for invoking Cla"}
{"path": "sentinel_core/adapters/gemini_adapter.py", "size_bytes": 249, "lines": 5, "lang": "python", "sha256": "f0feb143974fcb673960054fe46449ae4e40d2061ee27e5dd476f36e4812a7df", "mtime_iso": "2025-10-01T08:02:51.574641+00:00", "preview": "class GeminiAdapter:\n    def suggest_patch(self, diagnosis: str) -> str:\n        \"\"\"Placeholder for suggesting a patch with Gemini.\"\"\"\n        print(f\"Suggesting patch with Gemini for diagnosis: {diagnosis}\")\n        return f\"Patch for: {diagnosis}\""}
{"path": "sentinel_core/adapters/grok_adapter.py", "size_bytes": 130, "lines": 4, "lang": "python", "sha256": "1a3a2315db10f2bb6bbfbae8a65dcba83f47771f56350caf0542c2a6a23a2a64", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "class GrokAdapter:\n    def __init__(self):\n        \"\"\"Placeholder for Grok adapter.\"\"\"\n        print(\"Initializing Grok adapter.\")"}
{"path": "sentinel_core/adapters/openai_adapter.py", "size_bytes": 136, "lines": 4, "lang": "python", "sha256": "ca34b49d359b4997b3c904b284ec28e8b289c9343e7fb63e30ca969025a12d47", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "class OpenAIAdapter:\n    def __init__(self):\n        \"\"\"Placeholder for OpenAI adapter.\"\"\"\n        print(\"Initializing OpenAI adapter.\")"}
{"path": "sentinel_core/agents/anti_slop_benchmark.py", "size_bytes": 2090, "lines": 56, "lang": "python", "sha256": "b96e313b0d4aa2e993ba9a8dddce6e81828c451e2c01e243322c05de836072ee", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "import re\n\nclass AntiSlopBenchmark:\n    def __init__(self):\n        \"\"\"Initializes the AntiSlopBenchmark.\"\"\"\n        # More sophisticated models can be loaded here\n        pass\n\n    def _calculate_em_dash_density(self, text: str) -> float:\n        \"\"\"Calculates the density of em-dashes.\"\"\"\n        if not text:\n            return 0.0\n        return text.count(\"—\") / len(text)\n\n    def _calculate_enthusiasm_inflation(self, text: str) -> float:\n        \"\"\"Scores the text for excessive enthusiasm.\"\"\"\n        "}
{"path": "sentinel_core/agents/consensus_engine.py", "size_bytes": 554, "lines": 15, "lang": "python", "sha256": "5e916f37ec9d1b7a35f872b7af81c80fc2658d64e364f3cd3f6e73e487b4797f", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "class ConsensusEngine:\n    def __init__(self):\n        \"\"\"Initializes the ConsensusEngine.\"\"\"\n        pass\n\n    def score_agreement(self, outputs: list) -> dict:\n        \"\"\"\n        Scores the agreement between outputs from multiple models.\n\n        :param outputs: A list of outputs from different models.\n        :return: A dictionary containing the agreement score.\n        \"\"\"\n        print(f\"Scoring agreement for {len(outputs)} model outputs.\")\n        # Placeholder for consensus logic\n        return {\"ag"}
{"path": "sentinel_core/agents/cortex.py", "size_bytes": 327, "lines": 11, "lang": "python", "sha256": "37039f54ca271336483a541f72c167ac474d5c99b6a89781f893cf21a4d7953b", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "\"\"\"\nCortex: The reasoning and orchestration layer for the agentic system.\n\"\"\"\n\nclass Cortex:\n    def __init__(self):\n        print(\"Initializing Cortex.\")\n\n    def orchestrate(self, signal: dict):\n        \"\"\"Placeholder for orchestrating a response to a signal.\"\"\"\n        print(f\"Cortex is orchestrating for signal: {signal}\")"}
{"path": "sentinel_core/agents/jules.py", "size_bytes": 4148, "lines": 96, "lang": "python", "sha256": "7d270f7d1bc1b75dfdf51c0b0d9b6dbc692f3b55a36c1834059c375211de4b2f", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "\"\"\"\nJules: An extremely skilled software engineer agent that learns and evolves.\n\"\"\"\nfrom .anti_slop_benchmark import AntiSlopBenchmark\nfrom .consensus_engine import ConsensusEngine\nfrom .patch_success_predictor import PatchSuccessPredictor\nfrom .self_evolving_memory import SelfEvolvingMemory\nfrom .symbolic_consistency_verifier import SymbolicConsistencyVerifier\nfrom ..storage import neo4j_anchor as graph_db\n\n\nclass JulesAgent:\n    def __init__(self):\n        \"\"\"Initializes the Jules agent with all its comp"}
{"path": "sentinel_core/agents/patch_success_predictor.py", "size_bytes": 2214, "lines": 62, "lang": "python", "sha256": "e263f9304ddcedbdce0d7f2f3a622ae612414d6f52fef5d8335b2d2c363f10c9", "mtime_iso": "2025-10-01T08:02:51.578641+00:00", "preview": "import random\n\nclass PatchSuccessPredictor:\n    def __init__(self):\n        \"\"\"\n        Initializes the PatchSuccessPredictor.\n        In a real implementation, this would load a pre-trained model.\n        \"\"\"\n        # self.model = load_model(\"path/to/pr_success_model.pkl\")\n        pass\n\n    def _extract_features(self, pr_data: dict) -> dict:\n        \"\"\"\n        Extracts features from PR data to be used for prediction.\n        This simulates fetching data about the contributor, PR size, etc.\n        \"\"\"\n  "}
{"path": "sentinel_core/agents/self_evolving_memory.py", "size_bytes": 3256, "lines": 88, "lang": "python", "sha256": "5de27982d6797db890ac247a834ce72927712a678d762c930a97c3de5f285482", "mtime_iso": "2025-10-01T08:02:51.582641+00:00", "preview": "import json\nimport os\nimport time\nfrom collections import Counter\n\nclass SelfEvolvingMemory:\n    def __init__(self, path=\"/opt/jules/memory.json\"):\n        \"\"\"Initializes the SelfEvolvingMemory.\"\"\"\n        self.path = path\n        self.cache = {}\n        self.current_strategy = {\"name\": \"default\", \"confidence\": 0.5}\n        self.load()\n\n    def load(self):\n        \"\"\"Loads the memory cache and strategy from a file.\"\"\"\n        if os.path.exists(self.path):\n            try:\n                with open(self.path"}
{"path": "sentinel_core/agents/sentinel.py", "size_bytes": 826, "lines": 20, "lang": "python", "sha256": "2765fca0082c0e5eba40c67861c70830e9e482ffaa0ea3bfcd9f4ac483ba4c97", "mtime_iso": "2025-10-01T08:02:51.582641+00:00", "preview": "from ..adapters.claude_adapter import ClaudeAdapter\nfrom ..adapters.gemini_adapter import GeminiAdapter\nfrom ..workflows import selfhealworkflow, validation_workflow\nfrom ..storage.neo4j_anchor import anchor_event\n\nclass SentinelAgent:\n    def __init__(self):\n        self.claude = ClaudeAdapter()\n        self.gemini = GeminiAdapter()\n\n    def monitor(self, signal: dict):\n        if signal[\"type\"] == \"failure\":\n            diagnosis = self.claude.debug_traceback(signal[\"traceback\"])\n            patch = self."}
{"path": "sentinel_core/agents/symbolic_consistency_verifier.py", "size_bytes": 2843, "lines": 62, "lang": "python", "sha256": "4e5398f4e89bbb43a53883a5655c94388d48063e1bbbcbadcf9683f22bceb020", "mtime_iso": "2025-10-01T08:02:51.582641+00:00", "preview": "class SymbolicConsistencyVerifier:\n    def __init__(self):\n        \"\"\"Initializes the SymbolicConsistencyVerifier.\"\"\"\n        # In a real implementation, this would load models or knowledge graphs.\n        pass\n\n    def _get_scientific_framing(self, concept: str) -> dict:\n        \"\"\"Retrieves the scientific framing of a concept.\"\"\"\n        # Placeholder: In a real system, this would query a scientific database or LLM.\n        return {\"source\": \"scientific\", \"definition\": f\"A scientific definition of {concep"}
{"path": "sentinel_core/agents/validator.py", "size_bytes": 374, "lines": 12, "lang": "python", "sha256": "accee8bd972eab07a6d73c8938533d5efe88f94a338a1e699f4ad6b024e91e5e", "mtime_iso": "2025-10-01T08:02:51.582641+00:00", "preview": "\"\"\"\nValidator: An agent responsible for validating system state and patches.\n\"\"\"\n\nclass ValidatorAgent:\n    def __init__(self):\n        print(\"Initializing Validator agent.\")\n\n    def validate(self, target: str):\n        \"\"\"Placeholder for validating a target.\"\"\"\n        print(f\"Validator is validating target: {target}\")\n        return {\"status\": \"pass\", \"target\": target}"}
{"path": "sentinel_core/config/sentinel_config.py", "size_bytes": 222, "lines": 13, "lang": "python", "sha256": "572a9c1b8153e87f29026cca9e8ed2c3888a85610041383ea45ea4bcaa2147ac", "mtime_iso": "2025-10-01T08:02:51.582641+00:00", "preview": "\"\"\"\nConfiguration for SentinelCore.\n\"\"\"\n\nclass Config:\n    NEO4J_URI = \"bolt://localhost:7687\"\n    NEO4J_USER = \"neo4j\"\n    NEO4J_PASSWORD = \"password\"\n\n    REDIS_HOST = \"localhost\"\n    REDIS_PORT = 6379\n\nconfig = Config()"}
{"path": "sentinel_core/interfaces/cli.py", "size_bytes": 223, "lines": 15, "lang": "python", "sha256": "5d6b3a6f33cbd1afa91d8a3777a382d1dea4948ca7d83f166dbb76b090d0d4e6", "mtime_iso": "2025-10-01T08:02:51.586641+00:00", "preview": "\"\"\"\nCLI for SentinelCore.\n\"\"\"\n\nimport typer\n\napp = typer.Typer()\n\n@app.command()\ndef monitor():\n    \"\"\"Placeholder for monitoring from the CLI.\"\"\"\n    print(\"Monitoring for signals...\")\n\nif __name__ == \"__main__\":\n    app()"}
{"path": "sentinel_core/interfaces/mobile_api.py", "size_bytes": 227, "lines": 10, "lang": "python", "sha256": "295ed30585e3e66c20eb0378938bc0ac2234450506874b22167c7758efd99e1f", "mtime_iso": "2025-10-01T08:02:51.586641+00:00", "preview": "from fastapi import FastAPI\nfrom agents.sentinel import SentinelAgent\n\napp = FastAPI()\nagent = SentinelAgent()\n\n@app.post(\"/signal\")\ndef receive_signal(signal: dict):\n    agent.monitor(signal)\n    return {\"status\": \"processed\"}"}
{"path": "sentinel_core/interfaces/web_api.py", "size_bytes": 169, "lines": 11, "lang": "python", "sha256": "9a91821657230d1a0900034ea26a33942595f7290f932b9deab2ba230ba0a602", "mtime_iso": "2025-10-01T08:02:51.586641+00:00", "preview": "\"\"\"\nWeb API for SentinelCore.\n\"\"\"\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Welcome to SentinelCore Web API\"}"}
{"path": "sentinel_core/main.py", "size_bytes": 193, "lines": 9, "lang": "python", "sha256": "54f477ae845685e5cbad6b810417b4a3e884a1268a5ff3a52f9b4e235f480d11", "mtime_iso": "2025-10-01T08:02:51.586641+00:00", "preview": "\"\"\"\nMain entry point for the SentinelCore application.\n\"\"\"\n\nfrom .interfaces.mobile_api import app\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"}
{"path": "sentinel_core/storage/audit_logger.py", "size_bytes": 172, "lines": 7, "lang": "python", "sha256": "59a0cf07c3d4a65061a6f3646bb0b00a1bcfba47cd8a837a1984d5f063b27ddc", "mtime_iso": "2025-10-01T08:02:51.586641+00:00", "preview": "\"\"\"\nAudit Logger: For logging critical events and actions.\n\"\"\"\n\ndef log_event(event: dict):\n    \"\"\"Placeholder for logging an event.\"\"\"\n    print(f\"Logging event: {event}\")"}
{"path": "sentinel_core/storage/memory_cache.py", "size_bytes": 584, "lines": 19, "lang": "python", "sha256": "c9f68356463c58eea2afe91e53442c8aaa4373436ffe42829f2c3bce2f87ca8c", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "\"\"\"\nMemory Cache: For in-memory storage and caching (e.g., Redis).\n\"\"\"\n\nclass MemoryCache:\n    def __init__(self):\n        self.cache = {}\n        print(\"Initializing Memory Cache.\")\n\n    def set(self, key: str, value: any):\n        \"\"\"Placeholder for setting a value in the cache.\"\"\"\n        print(f\"Setting cache key '{key}' to '{value}'\")\n        self.cache[key] = value\n\n    def get(self, key: str) -> any:\n        \"\"\"Placeholder for getting a value from the cache.\"\"\"\n        value = self.cache.get(key)\n   "}
{"path": "sentinel_core/storage/neo4j_anchor.py", "size_bytes": 3058, "lines": 71, "lang": "python", "sha256": "4783a50264384c0f7ba1fa719d39fc449dea518a15d16cdf3b841264f3374640", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "# Placeholder for the Neo4j driver\nclass Neo4jDriver:\n    def session(self):\n        return self\n\n    def run(self, query, **params):\n        # Reformatting params for printing\n        print(f\"Executing query: {query} with params: {params}\")\n        return self\n\n    def single(self):\n        return self\n\n    def data(self):\n        return {}\n\n# In a real application, this would be properly configured.\ndriver = Neo4jDriver()\n\ndef anchor_jules_task(task_id: str, description: str, concept: str = None):\n    \"\"\""}
{"path": "sentinel_core/workflows/jules_activities.py", "size_bytes": 452, "lines": 10, "lang": "python", "sha256": "69d9a09fcc8a1a4c3ce074ffe877bbc1c9072e113057220be9a154d0b13fcf81", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "from ..adapters.claude_adapter import ClaudeAdapter\nfrom .patch_generator import synthesize_patch\nfrom .merge_resolver import resolveconflicts\nfrom .pr_drafter import createpatchpr\n\ndef autodebugandpatchcifailure(traceback: str, prbranch: str):\n    diagnosis = ClaudeAdapter().diagnose_build_failure(traceback)\n    patch = synthesize_patch(diagnosis)\n    resolveconflicts(prbranch)\n    return createpatchpr(patch, \"fix: install g++ for pybullet build\")"}
{"path": "sentinel_core/workflows/merge_resolver.py", "size_bytes": 339, "lines": 11, "lang": "python", "sha256": "de8a2a4f718c7c52dee21f61c21c01eea3778f0ded330b7865f0c336217e4a1f", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "import subprocess\n\ndef run(command: str):\n    \"\"\"Placeholder for running a command.\"\"\"\n    print(f\"Running command: {command}\")\n    subprocess.run(command, shell=True)\n\ndef resolveconflicts(pr_branch: str):\n    run(f\"git checkout {pr_branch}\")\n    run(\"git merge origin/main\")\n    run(\"git mergetool || git commit -am 'Resolve conflicts'\")"}
{"path": "sentinel_core/workflows/patch_generator.py", "size_bytes": 228, "lines": 7, "lang": "python", "sha256": "1f9e3acb4e6d479220bf6c91572d6bd340dbad53ef1c99166b6b969101403695", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "def synthesize_patch(diagnosis: str) -> dict:\n    if \"g++\" in diagnosis:\n        return {\n            \"file\": \"Dockerfile\",\n            \"insert\": \"RUN apt-get update && apt-get install -y build-essential\"\n        }\n    return {}"}
{"path": "sentinel_core/workflows/patch_workflow.py", "size_bytes": 371, "lines": 11, "lang": "python", "sha256": "d22ed484fd857d6333df2417fb7eaa3985b497560518777ef71d5a2082c48687", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "\"\"\"\nWorkflow for applying patches.\n\"\"\"\n\ndef apply_patch(patch: dict):\n    \"\"\"Placeholder for applying a patch.\"\"\"\n    print(f\"Applying patch: {patch}\")\n    # In a real implementation, this would modify the file.\n    if \"file\" in patch and \"insert\" in patch:\n        with open(patch[\"file\"], \"a\") as f:\n            f.write(f\"\\n# Auto-generated patch\\n{patch['insert']}\\n\")"}
{"path": "sentinel_core/workflows/pr_drafter.py", "size_bytes": 981, "lines": 26, "lang": "python", "sha256": "b281c1c5dcac7607fefd9477a06da36687643108e9055188976e328cbedd2e97", "mtime_iso": "2025-10-01T08:02:51.590641+00:00", "preview": "import subprocess\n\ndef run(command: str):\n    \"\"\"Placeholder for running a command.\"\"\"\n    print(f\"Running command: {command}\")\n    subprocess.run(command, shell=True)\n\ndef apply_patch(patch: dict):\n    \"\"\"Placeholder for applying a patch.\"\"\"\n    print(f\"Applying patch: {patch}\")\n    # In a real implementation, this would modify the file.\n    if \"file\" in patch and \"insert\" in patch:\n        with open(patch[\"file\"], \"a\") as f:\n            f.write(f\"\\n# Auto-generated patch\\n{patch['insert']}\\n\")\n\ndef create"}
{"path": "sentinel_core/workflows/selfhealworkflow.py", "size_bytes": 502, "lines": 16, "lang": "python", "sha256": "7221fd3adf4b1266fb4a8d91a53257b4ca6ef4dd2ec9dec731697c75b184f703", "mtime_iso": "2025-10-01T08:02:51.594641+00:00", "preview": "def apply_patch(patch: str):\n    \"\"\"Placeholder for applying a patch.\"\"\"\n    print(f\"Applying patch: {patch}\")\n\ndef revalidate():\n    \"\"\"Placeholder for revalidation.\"\"\"\n    print(\"Revalidating...\")\n\ndef logtoaudit(message: str, diagnosis: str, patch: str):\n    \"\"\"Placeholder for audit logging.\"\"\"\n    print(f\"AUDIT: {message}, Diagnosis: {diagnosis}, Patch: {patch}\")\n\ndef run(diagnosis: str, patch: str):\n    apply_patch(patch)\n    revalidate()\n    logtoaudit(\"Self-heal executed\", diagnosis, patch)"}
{"path": "sentinel_core/workflows/validation_workflow.py", "size_bytes": 221, "lines": 9, "lang": "python", "sha256": "07b9e5026f5f5111e8d403598a5337f1f1a77c0aac6bef530916890e25842eec", "mtime_iso": "2025-10-01T08:02:51.594641+00:00", "preview": "from datetime import datetime\n\ndef run(target: str) -> dict:\n    # Run semantic, structural, and policy validation\n    return {\n        \"target\": target,\n        \"status\": \"pass\",\n        \"timestamp\": datetime.now()\n    }"}
{"path": "services/palantir_harvester/Dockerfile", "size_bytes": 337, "lines": 15, "lang": "text", "sha256": "e0cd5549b01cdb3d196bc5b002774859c377490d439589ea185c3033fced494b", "mtime_iso": "2025-10-01T08:02:51.594641+00:00", "preview": "# Use the official Python image.\nFROM python:3.11-slim\n\n# Set the working directory.\nWORKDIR /app\n\n# Copy the requirements file and install dependencies.\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code.\nCOPY . .\n\n# Command to run the worker.\nCMD [\"python\", \"worker.py\"]"}
{"path": "services/palantir_harvester/activities/palantir_ingestion.py", "size_bytes": 392, "lines": 12, "lang": "python", "sha256": "f34e56d00bab2e3cd0bc3bf92171738f77ff175c467b29edafc5fe82c781d3f9", "mtime_iso": "2025-10-01T08:02:51.594641+00:00", "preview": "from temporalio import activity\n\n\n@activity.defn\nasync def ingest_palantir_features() -> str:\n    \"\"\"\n    Ingests Palantir's public AI features.\n    \"\"\"\n    activity.logger.info(\"Ingesting Palantir features...\")\n    # In a real implementation, this would connect to Palantir's public\n    # APIs or scrape their website to fetch the data.\n    return \"Successfully ingested Palantir features.\"\n"}
{"path": "services/palantir_harvester/requirements.txt", "size_bytes": 10, "lines": 1, "lang": "text", "sha256": "3c362ed0ec33d74d96388006acd0243c76fd2d01cf3c05acaab4c166551cf292", "mtime_iso": "2025-10-01T08:02:51.594641+00:00", "preview": "temporalio"}
{"path": "services/palantir_harvester/worker.py", "size_bytes": 603, "lines": 23, "lang": "python", "sha256": "4ace807d2cf02034d27217571b5ce970b77e62c9f565b6ca85ed32bf993c028b", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "import asyncio\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom .workflows.palantir_feature_harvester_workflow import (\n    PalantirFeatureHarvesterWorkflow,\n)\nfrom .activities.palantir_ingestion import ingest_palantir_features\n\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"palantir-harvester-task-queue\",\n        workflows=[PalantirFeatureHarvesterWorkflow],\n        activities=[ingest_palantir_fe"}
{"path": "services/palantir_harvester/workflows/palantir_feature_harvester_workflow.py", "size_bytes": 435, "lines": 15, "lang": "python", "sha256": "cb25716343dec39294d844a01c133fa037d16e5b67922d278e4f006fad59696d", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from ..activities.palantir_ingestion import ingest_palantir_features\n\n\n@workflow.defn\nclass PalantirFeatureHarvesterWorkflow:\n    @workflow.run\n    async def run(self):\n        return await workflow.execute_activity(\n            ingest_palantir_features,\n            start_to_close_timeout=timedelta(hours=4),\n        )\n"}
{"path": "services/requirements.txt", "size_bytes": 47, "lines": 5, "lang": "text", "sha256": "832ba375c30e3df3265e01dbf67571d817b9f8d6b6330616568a07f14d3b7e0d", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "pymilvus\nopenai\npsycopg2-binary\nuvicorn\nfastapi"}
{"path": "services/self_healing/Dockerfile", "size_bytes": 337, "lines": 15, "lang": "text", "sha256": "e0cd5549b01cdb3d196bc5b002774859c377490d439589ea185c3033fced494b", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "# Use the official Python image.\nFROM python:3.11-slim\n\n# Set the working directory.\nWORKDIR /app\n\n# Copy the requirements file and install dependencies.\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code.\nCOPY . .\n\n# Command to run the worker.\nCMD [\"python\", \"worker.py\"]"}
{"path": "services/self_healing/activities/bug_bounty_ingestion.py", "size_bytes": 409, "lines": 12, "lang": "python", "sha256": "f16959b92e7711d84b26cbba271860708ec4c3688e172d1e7dadd8ca4e3edff5", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "from temporalio import activity\n\n\n@activity.defn\nasync def ingest_bug_bounty_data() -> str:\n    \"\"\"\n    Ingests bug bounty data from HackerOne, Bugcrowd, and CVE disclosures.\n    \"\"\"\n    activity.logger.info(\"Ingesting bug bounty data...\")\n    # In a real implementation, this would connect to the APIs of the bug\n    # bounty platforms and fetch the data.\n    return \"Successfully ingested bug bounty data.\"\n"}
{"path": "services/self_healing/requirements.txt", "size_bytes": 10, "lines": 1, "lang": "text", "sha256": "3c362ed0ec33d74d96388006acd0243c76fd2d01cf3c05acaab4c166551cf292", "mtime_iso": "2025-10-01T08:02:51.598641+00:00", "preview": "temporalio"}
{"path": "services/self_healing/worker.py", "size_bytes": 546, "lines": 21, "lang": "python", "sha256": "cd16492c089a4a3260e14c25d536874cdea9d1946f5542dd2eff7e4753c38fee", "mtime_iso": "2025-10-01T08:02:51.602641+00:00", "preview": "import asyncio\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom .workflows.self_healing_workflow import SelfHealingWorkflow\nfrom .activities.bug_bounty_ingestion import ingest_bug_bounty_data\n\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"self-healing-task-queue\",\n        workflows=[SelfHealingWorkflow],\n        activities=[ingest_bug_bounty_data],\n    )\n    await worker.run()\n\n\nif __name__ == \"_"}
{"path": "services/self_healing/workflows/self_healing_workflow.py", "size_bytes": 422, "lines": 15, "lang": "python", "sha256": "b8af4b887bda61d8c6fed0c2a67080eec7846111b287c436b8c652935df2025a", "mtime_iso": "2025-10-01T08:02:51.602641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from ..activities.bug_bounty_ingestion import ingest_bug_bounty_data\n\n\n@workflow.defn\nclass SelfHealingWorkflow:\n    @workflow.run\n    async def run(self):\n        return await workflow.execute_activity(\n            ingest_bug_bounty_data,\n            start_to_close_timeout=timedelta(minutes=5),\n        )\n"}
{"path": "services/semantic-search-api/main.py", "size_bytes": 4402, "lines": 138, "lang": "python", "sha256": "3d4cde94bb9694b95e2632ebbf1ea6afb02dab3f2ec8ddb0fbe1bdd70e945866", "mtime_iso": "2025-10-01T08:02:51.602641+00:00", "preview": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom pymilvus import connections, Collection\nfrom openai import AsyncOpenAI\nimport os\nimport json\n\n# --- Configuration (Mirrors embedding_activities.py) ---\nclient = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nMILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"milvus-proxy.synapse-system.svc.cluster.local\")\nMILVUS_PORT = \"19530\"\n\n# High-Fidelity Model\nEMBEDDING_MODEL = \"text-embedding-3-large\"\nEMBEDDING_DIM = 3072\n\n# --- FastAPI Application"}
{"path": "services/semantic_search_api/main.py", "size_bytes": 3676, "lines": 125, "lang": "python", "sha256": "15db5ff400fd0e8964b7915123ec0ad48ff157323bf058ecef04e4f45e926ee1", "mtime_iso": "2025-10-01T08:02:51.602641+00:00", "preview": "# services/semantic-search-api/main.py\nimport os\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI, HTTPException\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom pymilvus import Collection, connections\nfrom temporalio.client import Client\n\nfrom services.synapse_cortex.activities import send_prompt_to_jules\n\n# Use an async client for a FastAPI app\nclient = AsyncOpenAI()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Connect to Milvus on startup\n    con"}
{"path": "services/synapse-cortex/activities/embedding_activities.py", "size_bytes": 7572, "lines": 221, "lang": "python", "sha256": "16c104b9236846946aa8bc297871fdd89a2318056e97754472e291597146d317", "mtime_iso": "2025-10-01T08:02:51.606641+00:00", "preview": "# services/synapse-cortex/activities/embedding_activities.py\nimport os\nfrom datetime import datetime\n\nimport psycopg2\nfrom openai import OpenAI\nfrom pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections\nfrom temporalio import activity\n\n# Initialize OpenAI client. It will use the OPENAI_API_KEY environment variable.\nclient = OpenAI()\n\n\ndef _get_db_connection():\n    \"\"\"Establishes a database connection using credentials from environment variables.\"\"\"\n    try:\n        return psycopg2"}
{"path": "services/synapse-cortex/worker.py", "size_bytes": 1653, "lines": 48, "lang": "python", "sha256": "ff727c3a6c377fe76e609e888758608710408d1992bb319efc4bdd7986a9d327", "mtime_iso": "2025-10-01T08:02:51.606641+00:00", "preview": "import asyncio\nimport os\n\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\n# Import the workflow and activities to register them with the worker\nfrom services.synapse_cortex.activities import embedding_activities\nfrom services.synapse_cortex.workflows.semantic_search_workflow import (\n    SemanticSearchWorkflow,\n)\n\n\nasync def main():\n    \"\"\"The main entry point for the Temporal worker.\"\"\"\n    # Connect to the Temporal cluster.\n    # The address is read from an environment variable "}
{"path": "services/synapse-cortex/workflows/semantic_search_workflow.py", "size_bytes": 3509, "lines": 103, "lang": "python", "sha256": "221203725de4be9b46bea951a63af1e614902f9ebe2f6d1c2f124b3b0af2797c", "mtime_iso": "2025-10-01T08:02:51.606641+00:00", "preview": "import asyncio\nfrom datetime import timedelta\n\nfrom temporalio import workflow\n\n# Import activity stubs. The actual implementations will be in the activities file.\n# The `with` block is necessary for the Temporal runtime to correctly handle imports.\nwith workflow.unsafe.imports_passed_through():\n    from services.synapse_cortex.activities.embedding_activities import (\n        fetch_note_points,\n        fetch_citations,\n        fetch_audit_events,\n        generate_embeddings,\n        index_to_milvus,\n       "}
{"path": "services/synapse_cortex/activities/__init__.py", "size_bytes": 125, "lines": 4, "lang": "python", "sha256": "885262d2df5d00d7e8ef8cd2ae4f662266ade534351d7bc7ef8c235fc5dcde8a", "mtime_iso": "2025-10-01T08:02:51.606641+00:00", "preview": "# services/synapse_cortex/activities/__init__.py\nfrom .prompt import send_prompt_to_jules\n\n__all__ = [\"send_prompt_to_jules\"]"}
{"path": "services/synapse_cortex/activities/embedding_activities.py", "size_bytes": 6551, "lines": 201, "lang": "python", "sha256": "408025b928fa568e4ada133d2892fe687aa39eb1588642ea3c712a019789ad00", "mtime_iso": "2025-10-01T08:02:51.606641+00:00", "preview": "from temporalio import activity\nfrom pymilvus import (\n    connections,\n    Collection,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    utility,\n)\nfrom openai import AsyncOpenAI\nimport os\nimport psycopg2\nimport json\n\n# Configuration\nclient = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nMILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"milvus-proxy.synapse-system.svc.cluster.local\")\nDB_CONN_STRING = os.getenv(\"DB_CONN_STRING\")\n\n# High-Fidelity Model\nEMBEDDING_MODEL = \"text-embedding-3-large\"\nEMBEDDING_DIM"}
{"path": "services/synapse_cortex/activities/policy_activities.py", "size_bytes": 1781, "lines": 42, "lang": "python", "sha256": "8e71dc6c01bb55b4d4130766662c639f0a528a428a8c513e125a0402c89fa2eb", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "import httpx\nfrom temporalio import activity\nimport os\n\n# OPA service endpoint within the K8s cluster (deployed via Helm)\n# The service name deployed by the Helm chart is typically 'opa'.\nOPA_URL = os.getenv(\"OPA_URL\", \"http://opa.synapse-system.svc.cluster.local:8181\")\n\n\n@activity.defn\nasync def evaluate_policy(policy_path: str, input_data: dict) -> dict:\n    \"\"\"\n    Queries the OPA service to evaluate a policy using an async HTTP client.\n\n    Args:\n        policy_path: The dot-separated path to the policy"}
{"path": "services/synapse_cortex/activities/prompt.py", "size_bytes": 558, "lines": 14, "lang": "python", "sha256": "c39a6f2ba78f0895e7c74f522d855cf1b7cf6e5a983669528426bc269914cce9", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "# services/synapse_cortex/activities/prompt.py\nfrom temporalio import activity\n\n@activity.defn\ndef send_prompt_to_jules(payload: dict) -> dict:\n    \"\"\"\n    An activity to send a prompt to the Jules agent.\n\n    This is a scaffold and currently just logs the received payload.\n    In a real implementation, this would trigger a call to the Jules agent.\n    \"\"\"\n    activity.logger.info(f\"Received prompt payload: {payload}\")\n    # In a real scenario, you might return a confirmation or result\n    return {\"status\":"}
{"path": "services/synapse_cortex/workflows/issue_to_pr.py", "size_bytes": 3002, "lines": 75, "lang": "python", "sha256": "b4badd838d0a4ea25cbc0cd5a317fe32836baf0b1ec8a6487b2796318d78dec2", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import workflow\n\n# Import activity stubs\nfrom services.synapse_cortex.activities.policy_activities import evaluate_policy\n\n# Mock other activities for this example\n# from .activities import ensure_audit_anchor, dispatch_jules_merge, get_scan_report\n\n\n# Since other activities are not defined, we'll create mock stubs for them\n# In a real scenario, these would be imported from their respective activity files.\n@workflow.activity\nasync def ensure_audit_anchor(data: "}
{"path": "services/synapse_cortex/workflows/semantic-search.ts", "size_bytes": 2315, "lines": 66, "lang": "typescript", "sha256": "85db0d814cbc3ae8371e8f5b0a63ea67c9d4a876618c1125a9cfff12eb562adf", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "import { proxyActivities, workflow } from '@temporalio/workflow';\nimport type * as activities from 'services/synapse_cortex/activities/embedding_activities';\n\nconst {\n  fetch_note_points,\n  fetch_citations,\n  // fetch_audit_events,\n  generate_embeddings,\n  index_to_milvus,\n} = proxyActivities<typeof activities>({\n  startToCloseTimeout: '45 minutes', // Extended timeout for large batches\n  retry: { initialInterval: '30s', maximumAttempts: 5 },\n});\n\n// ... (Interface definitions for EmbeddingBatch and Embeddi"}
{"path": "services/xbow_validator/Dockerfile", "size_bytes": 337, "lines": 15, "lang": "text", "sha256": "e0cd5549b01cdb3d196bc5b002774859c377490d439589ea185c3033fced494b", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "# Use the official Python image.\nFROM python:3.11-slim\n\n# Set the working directory.\nWORKDIR /app\n\n# Copy the requirements file and install dependencies.\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code.\nCOPY . .\n\n# Command to run the worker.\nCMD [\"python\", \"worker.py\"]"}
{"path": "services/xbow_validator/activities/penetration_testing.py", "size_bytes": 457, "lines": 12, "lang": "python", "sha256": "7708776fd8cb64531b2f6f2c6dbf64572cceadc9826590d21eef25a4c7e327fe", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "from temporalio import activity\n\n\n@activity.defn\nasync def simulate_penetration_test(patch_id: str) -> str:\n    \"\"\"\n    Simulates a penetration test for a given patch.\n    \"\"\"\n    activity.logger.info(f\"Simulating penetration test for patch {patch_id}...\")\n    # In a real implementation, this would use synthetic payloads and\n    # fuzzing agents to validate the patch integrity.\n    return f\"Successfully simulated penetration test for patch {patch_id}.\"\n"}
{"path": "services/xbow_validator/requirements.txt", "size_bytes": 10, "lines": 1, "lang": "text", "sha256": "3c362ed0ec33d74d96388006acd0243c76fd2d01cf3c05acaab4c166551cf292", "mtime_iso": "2025-10-01T08:02:51.610641+00:00", "preview": "temporalio"}
{"path": "services/xbow_validator/worker.py", "size_bytes": 562, "lines": 21, "lang": "python", "sha256": "11090164d6a6c2cd8efe4eba1791a12c19ff0e7d61e05f6e50f84f1b2c7681dc", "mtime_iso": "2025-10-01T08:02:51.614641+00:00", "preview": "import asyncio\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom .workflows.xbow_validation_workflow import XbowValidationWorkflow\nfrom .activities.penetration_testing import simulate_penetration_test\n\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"xbow-validator-task-queue\",\n        workflows=[XbowValidationWorkflow],\n        activities=[simulate_penetration_test],\n    )\n    await worker.run()\n\n\ni"}
{"path": "services/xbow_validator/workflows/xbow_validation_workflow.py", "size_bytes": 465, "lines": 16, "lang": "python", "sha256": "5df61b415989299372d4bb8766257dd73e56bd21a8c456b42dc81452cfdf9b2f", "mtime_iso": "2025-10-01T08:02:51.614641+00:00", "preview": "from datetime import timedelta\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from ..activities.penetration_testing import simulate_penetration_test\n\n\n@workflow.defn\nclass XbowValidationWorkflow:\n    @workflow.run\n    async def run(self, patch_id: str):\n        return await workflow.execute_activity(\n            simulate_penetration_test,\n            patch_id,\n            start_to_close_timeout=timedelta(hours=1),\n        )\n"}
{"path": "setup.py", "size_bytes": 1434, "lines": 45, "lang": "python", "sha256": "0896004d26e1e2681ba7c7cbf276a4cdd0a281a9e57fffa5cd6c1d612b01d838", "mtime_iso": "2025-10-01T08:11:33.818260+00:00", "preview": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"optimizer\",\n    version=\"0.1.0\",\n    description=\"Augmented optimizer for virtual node and game-engine authentication matrix simulation.\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    author=\"Jules\",\n    packages=find_packages(),\n    install_requires=[\n        \"fastapi\",\n        \"uvicorn[standard]\",\n        \"pydantic\",\n        \"pyyaml\",\n        \"click\",\n        \"pytest\",\n        \"flake8\",\n "}
{"path": "syzygy-os/flake.nix", "size_bytes": 1362, "lines": 38, "lang": "text", "sha256": "fe9801521cb98c4629ed6fbdf09cff385d53de2d5a43dd8b0ca40dc9568a36e8", "mtime_iso": "2025-10-01T08:02:51.614641+00:00", "preview": "{\n  description = \"SyzygyOS Immutable Infrastructure\";\n\n  inputs = {\n    # Use stable NixOS channel (24.05 as of late 2025)\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixos-24.05\";\n\n    # Home Manager for personalization (themes, dotfiles)\n    home-manager.url = \"github:nix-community/home-manager/release-24.05\";\n    home-manager.inputs.nixpkgs.follows = \"nixpkgs\";\n\n    # Generators for cloud images\n    nixos-generators.url = \"github:nix-community/nixos-generators\";\n    nixos-generators.inputs.nixpkgs.follows ="}
{"path": "syzygy-os/hosts/syzygy-gce/configuration.nix", "size_bytes": 1708, "lines": 58, "lang": "text", "sha256": "bf91fbca1549bd22e0148c3ffc9ec8d755aba2d9937e4e927985706558f0bb81", "mtime_iso": "2025-10-01T08:02:51.618641+00:00", "preview": "{ config, pkgs, inputs, ... }:\n\n{\n  imports =\n    [\n      # Import user configuration\n      ../../users/operator/home.nix\n    ];\n\n  # 1. Boot Configuration (UEFI required for GCE)\n  boot.loader.systemd-boot.enable = true;\n  boot.loader.efi.canTouchEfiVariables = true;\n\n  # 2. Networking & GCE Optimization (MANDATORY)\n  networking.hostName = \"syzygy-core\";\n  # Enable GCE network drivers and guest agents\n  nixpkgs.config.google-compute-image.enable = true;\n  services.google-guest-agent.enable = true;\n\n  # 3. "}
{"path": "syzygy-os/users/operator/home.nix", "size_bytes": 746, "lines": 30, "lang": "text", "sha256": "68b310e7c7f70b36058597a738c87a3d94cad51e7f425dfb61af116e7ab3f259", "mtime_iso": "2025-10-01T08:02:51.618641+00:00", "preview": "{ config, pkgs, ... }:\n\n{\n  # Integrate home-manager with the NixOS configuration\n  home-manager.users.operator = {\n    # 1. Shell Configuration (Zsh + Powerlevel10k Theme)\n    programs.zsh = {\n      enable = true;\n      oh-my-zsh = {\n        enable = true;\n        plugins = [ \"git\" \"kubectl\" \"docker\" \"terraform\" ];\n        # Use Powerlevel10k for advanced visual customization\n        theme = \"powerlevel10k/powerlevel10k\";\n      };\n    };\n\n    # 2. Custom Dotfiles (Example: Neovim with Gruvbox)\n    programs"}
{"path": "task_simulator/README.md", "size_bytes": 1164, "lines": 30, "lang": "markdown", "sha256": "4493ec91c64b9b69c9df8efc15dd0874a9ac9b74ea1e1f6cb65a72e0309f0710", "mtime_iso": "2025-10-01T08:02:51.618641+00:00", "preview": "# GitHub Issue Simulator\n\nThis script is designed to stress-test the Jules AGI agent by creating a configurable number of synthetic issues in a specified GitHub repository. It helps validate the entire task-handling pipeline, from ingestion to resolution.\n\n## Setup\n\n1.  **Install Dependencies:**\n    Navigate to the `task_simulator` directory and install the required Python packages.\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n## Usage\n\nRun the simulator from the command line, providing the nec"}
{"path": "task_simulator/requirements.txt", "size_bytes": 14, "lines": 2, "lang": "text", "sha256": "b33d62695da0b0a52bd4e5f3e8359cb0561df6460f8f937746e881feece8d4f1", "mtime_iso": "2025-10-01T08:02:51.618641+00:00", "preview": "PyGithub\nFaker"}
{"path": "task_simulator/simulator.py", "size_bytes": 2355, "lines": 65, "lang": "python", "sha256": "b096a6fbb7b190de08804eea30cff42ff323a030c431dfd61e6955a5e52ebfce", "mtime_iso": "2025-10-01T08:02:51.622641+00:00", "preview": "import os\nimport argparse\nimport random\nfrom github import Github\nfrom faker import Faker\n\n# --- Configuration ---\nLABELS_PER_ISSUE = {\n    \"bug\": [\"bug\", \"needs-triage\"],\n    \"feature\": [\"feature-request\", \"needs-discussion\"],\n    \"test\": [\"testing\", \"needs-review\"],\n    \"docs\": [\"documentation\", \"needs-clarification\"],\n}\nTASK_TYPES = list(LABELS_PER_ISSUE.keys())\n\n# --- Faker Initialization ---\nfake = Faker()\n\ndef generate_issue(repo_name, task_type):\n    \"\"\"Generates a synthetic GitHub issue.\"\"\"\n    titl"}
{"path": "tests/optimizer_sentinel/agents/test_predictive_debug_engine.py", "size_bytes": 2143, "lines": 53, "lang": "python", "sha256": "e1ed2522e3634820055b38eae802b7a8f6f3a0659087a936c1c35aa556f1d28b", "mtime_iso": "2025-10-01T08:02:51.622641+00:00", "preview": "import unittest\nfrom unittest.mock import MagicMock\n\n# Since the modules are in the parent directory, we might need to adjust the Python path.\n# For now, let's assume pytest handles it. If not, we'll fix it.\nfrom optimizer_sentinel.agents.predictive_debug_engine import PredictiveDebugEngine\nfrom optimizer_sentinel.adapters.risk_model import RiskModel\nfrom optimizer_sentinel.agents.patch_engine import PatchEngine\nfrom optimizer_sentinel.storage.neo4j_anchor import Neo4jAnchor\n\nclass TestPredictiveDebugEngine"}
{"path": "tests/optimizer_sentinel/test_schemamigrationguard.py", "size_bytes": 1016, "lines": 31, "lang": "python", "sha256": "1a777d0d475cf3304b73f0ecf244e42920ac34edb2695a3b5eb489fe74119a4f", "mtime_iso": "2025-10-01T08:02:51.622641+00:00", "preview": "import unittest\nfrom optimizer_sentinel.workflows.schemamigrationguard import SchemaMigrationGuard\n\nclass TestSchemaMigrationGuard(unittest.TestCase):\n\n    def setUp(self):\n        self.guard = SchemaMigrationGuard()\n\n    def test_validate_no_conflict(self):\n        \"\"\"\n        Test that a low-risk migration passes validation.\n        \"\"\"\n        target = {\"name\": \"add_index_to_users\", \"migration_risk\": \"low\"}\n        self.assertTrue(self.guard.validate(target))\n\n    def test_validate_with_conflict(self):\n "}
{"path": "tests/test_api.py", "size_bytes": 1900, "lines": 70, "lang": "python", "sha256": "b25257cce653ee23d2bc4d9fd8c1934b43a212e03237f6e3940c1c82d7c989ad", "mtime_iso": "2025-10-01T08:02:51.622641+00:00", "preview": "from fastapi.testclient import TestClient\nfrom optimizer.api.main import app\n\nclient = TestClient(app)\n\n\ndef test_health_check():\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"ok\"}\n\n\ndef test_ingest_node():\n    response = client.post(\n        \"/ingest/node\",\n        json={\n            \"node_id\": \"test_node\",\n            \"position\": [1, 2, 3],\n            \"metadata\": {\"key\": \"value\"},\n        },\n    )\n    assert response.status_code == 201\n   "}
{"path": "tests/test_build_helper.py", "size_bytes": 4601, "lines": 140, "lang": "python", "sha256": "c1753ab62474ca58a6255db78b9f3e36ec6c6a68008f41d3606a681df04bfb44", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "\"\"\"\nTests for the build helper utility.\n\"\"\"\n\nimport pytest\nfrom optimizer.utils.build_helper import (\n    DependencyChecker,\n    check_build_dependencies,\n    analyze_build_error,\n    BuildDependencyError,\n)\n\n\ndef test_dependency_checker_init():\n    \"\"\"Test DependencyChecker initialization.\"\"\"\n    checker = DependencyChecker()\n    assert checker.pkg_manager in [\"apt-get\", \"yum\", \"apk\", \"unknown\"]\n    assert isinstance(checker.missing_deps, list)\n\n\ndef test_command_exists():\n    \"\"\"Test that _command_exists "}
{"path": "tests/test_cli.py", "size_bytes": 1236, "lines": 44, "lang": "python", "sha256": "982d1b8948d61ff0c5fbcaf26ae6a6b5ebf6b0ead81b98ea0e8975fdb48225fc", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import logging\nfrom click.testing import CliRunner\nfrom optimizer.cli.main import cli\n\n\ndef test_cli_run_command():\n    runner = CliRunner()\n    result = runner.invoke(cli, [\"run\", \"--config-path\", \"nonexistent.yml\"])\n    assert result.exit_code != 0\n    assert \"Error: Configuration file not found at 'nonexistent.yml'\" in result.output\n\n\ndef test_cli_run_command_with_real_config():\n    # Reset logging to a clean state to prevent test pollution from other tests\n    # that might have configured file-based log"}
{"path": "tests/test_core.py", "size_bytes": 1429, "lines": 51, "lang": "python", "sha256": "152b3c28c592a320da8ee70bc95c1dda79e6a8e843a71432799623f0121cfbd9", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import pytest\nfrom optimizer.core.node import Node\nfrom optimizer.core.auth_matrix import AuthMatrix\nfrom optimizer.core.engine import Engine\n\n\ndef test_node_creation():\n    node = Node(node_id=\"test_node\", position=(1, 2, 3), metadata={\"info\": \"test\"})\n    assert node.node_id == \"test_node\"\n    assert node.position == (1, 2, 3)\n    assert node.metadata == {\"info\": \"test\"}\n\n\ndef test_node_to_dict():\n    node = Node(node_id=\"test_node\", position=(1, 2, 3))\n    assert node.to_dict() == {\n        \"node_id\": \"t"}
{"path": "tests/test_cross_platform_validator.py", "size_bytes": 3512, "lines": 80, "lang": "python", "sha256": "af59370bf508bcdf129151b9eb9efe3ebbebf62427e5f7229feb37cb3d207b2a", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import os\nimport stat\nimport subprocess\nfrom unittest.mock import patch, mock_open, AsyncMock, MagicMock\n\nimport pytest\n\n# Since `cross_platform_validator` is a proper package and the root is on the pythonpath,\n# these imports should work without any special path manipulation.\nfrom cross_platform_validator.validator import CrossPlatformValidator\nfrom cross_platform_validator.workflow import CrossPlatformValidationWorkflow, run_validation_activity\n\n# Mark all workflow/activity tests in this file as asyncio\np"}
{"path": "tests/test_jules_agent_suite.py", "size_bytes": 4694, "lines": 114, "lang": "python", "sha256": "ceae57eefd7ca1bcbbdac8a7db7d1790cc3a3be19c370980b2dc0b4f780b707a", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import unittest\nimport os\nimport json\nfrom unittest.mock import patch, MagicMock, call\n\nfrom sentinel_core.agents.anti_slop_benchmark import AntiSlopBenchmark\nfrom sentinel_core.agents.symbolic_consistency_verifier import SymbolicConsistencyVerifier\nfrom sentinel_core.agents.patch_success_predictor import PatchSuccessPredictor\nfrom sentinel_core.agents.self_evolving_memory import SelfEvolvingMemory\nfrom sentinel_core.agents.jules import JulesAgent\n\nclass TestAntiSlopBenchmark(unittest.TestCase):\n    def set"}
{"path": "tests/test_palantir_harvester.py", "size_bytes": 1023, "lines": 28, "lang": "python", "sha256": "7b16100e477ce48ae42fb5e52a36a23cbab914174614dcaad86957e2aa119855", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import pytest\nfrom temporalio.testing import WorkflowEnvironment\nfrom temporalio.worker import Worker\n\nfrom services.palantir_harvester.workflows.palantir_feature_harvester_workflow import (\n    PalantirFeatureHarvesterWorkflow,\n)\nfrom services.palantir_harvester.activities.palantir_ingestion import (\n    ingest_palantir_features,\n)\n\n\n@pytest.mark.asyncio\nasync def test_palantir_harvester_workflow():\n    task_queue = \"test-palantir-harvester-workflow\"\n    async with await WorkflowEnvironment.start_time_skip"}
{"path": "tests/test_self_healing.py", "size_bytes": 918, "lines": 24, "lang": "python", "sha256": "998a04abf1f06e78d68448ed00bb23004eaf090d45e9e93a61b5243e24766106", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import pytest\nfrom temporalio.testing import WorkflowEnvironment\nfrom temporalio.worker import Worker\n\nfrom services.self_healing.workflows.self_healing_workflow import SelfHealingWorkflow\nfrom services.self_healing.activities.bug_bounty_ingestion import ingest_bug_bounty_data\n\n\n@pytest.mark.asyncio\nasync def test_self_healing_workflow():\n    task_queue = \"test-self-healing-workflow\"\n    async with await WorkflowEnvironment.start_time_skipping() as env:\n        async with Worker(\n            env.client,\n   "}
{"path": "tests/test_semantic_search.py", "size_bytes": 3991, "lines": 117, "lang": "python", "sha256": "adfe69f8e3649305cd3955fba7052269c9bc8560d1d1f6c4aaef390d41a88d21", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "from unittest.mock import AsyncMock, MagicMock, patch\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\n# We need to set the environment variables before importing the app\nimport os\n\nos.environ[\"MILVUS_HOST\"] = \"mock-milvus\"\nos.environ[\"OPENAI_API_KEY\"] = \"mock-key\"\n\nfrom services.semantic_search_api.main import app\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef mock_openai():\n    \"\"\"Fixture to mock the OpenAI async client.\"\"\"\n    with patch(\n        \"services.semantic_search_api.main.client\", new_"}
{"path": "tests/test_xbow_validator.py", "size_bytes": 1073, "lines": 32, "lang": "python", "sha256": "336c090c416178a3d78386fb7cc3ec44cbf51f0c39a14f687c57fb29d2c2f27f", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "import pytest\nfrom temporalio.testing import WorkflowEnvironment\nfrom temporalio.worker import Worker\n\nfrom services.xbow_validator.workflows.xbow_validation_workflow import (\n    XbowValidationWorkflow,\n)\nfrom services.xbow_validator.activities.penetration_testing import (\n    simulate_penetration_test,\n)\n\n\n@pytest.mark.asyncio\nasync def test_xbow_validation_workflow():\n    task_queue = \"test-xbow-validation-workflow\"\n    async with await WorkflowEnvironment.start_time_skipping() as env:\n        async with"}
{"path": "tests/testcommitintegrity.py", "size_bytes": 3055, "lines": 76, "lang": "python", "sha256": "14af8f9d75fad0593bab7f9c46dce8c59327d2da642439f9eb59f3335a0da4d3", "mtime_iso": "2025-10-01T08:02:51.626641+00:00", "preview": "# FLAWMODE: Test Commit Integrity\n# This test suite verifies that the GitAutomator class correctly\n# prepares and simulates the Git operations for committing a patch.\n\nimport unittest\nfrom unittest.mock import patch, mock_open\nimport sys\n\n# Add the root directory to the path to allow importing the scripts\nsys.path.append('.')\n\nfrom scripts.commitandmerge import GitAutomator\n\nclass TestCommitIntegrity(unittest.TestCase):\n    \"\"\"\n    Tests the GitAutomator's logic for creating commits and PRs.\n    \"\"\"\n\n    de"}
{"path": "tests/testflawmaptrace.py", "size_bytes": 2827, "lines": 70, "lang": "python", "sha256": "93cfbc82b6268f9fa241facebab79237ea38eeb1a8350a2bf595f36fb9116c13", "mtime_iso": "2025-10-01T08:02:51.630641+00:00", "preview": "# FLAWMODE: Test Flaw Map Trace\n# This test suite verifies that the FlawRunner correctly selects,\n# executes, and traces a flaw injection event.\n\nimport unittest\nfrom unittest.mock import patch\nimport sys\n\n# Add the root directory to the path to allow importing the scripts\nsys.path.append('.')\n\nfrom scripts.julesflawmaprunner import FlawRunner\n\nclass TestFlawMapTrace(unittest.TestCase):\n    \"\"\"\n    Tests the functionality of the FlawRunner.\n    \"\"\"\n\n    def test_flaw_selection(self):\n        \"\"\"\n        Ens"}
{"path": "tests/testrecoverycycle.py", "size_bytes": 3330, "lines": 84, "lang": "python", "sha256": "b7d90d4269b9ed9dc2878756f38af00b4f6f937516ca40f6c43d3f705ba02cee", "mtime_iso": "2025-10-01T08:02:51.630641+00:00", "preview": "# FLAWMODE: Test Recovery Cycle\n# This test suite verifies the end-to-end functionality of the SelfHealingLoop.\n# It ensures the agent can process a flaw, generate a patch, and approve it.\n\nimport unittest\nfrom unittest.mock import MagicMock, patch\nimport sys\n\n# Add the root directory to the path to allow importing the scripts\nsys.path.append('.')\n\nfrom scripts.selfhealingloop import SelfHealingLoop\n\nclass TestRecoveryCycle(unittest.TestCase):\n    \"\"\"\n    Tests the full \"trace, patch, benchmark, anchor\" cyc"}
